{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LatiefDataVisionary/demand-forecasting-retail-college-task/blob/main/old_tpm_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "810d4a67"
      },
      "source": [
        "# **Demand Forecasting for Retail: A Machine Learning Pipeline Approach**\n",
        "\n",
        "## **Project Overview**\n",
        "\n",
        "**College Course:** Model Development Engineering\n",
        "\n",
        "**Objective:** To develop a robust machine learning model to accurately forecast daily product demand for a retail company. This involves a comprehensive data pipeline, including merging multiple data sources, extensive feature engineering, and model comparison to optimize inventory management and reduce costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98878728"
      },
      "source": [
        "## **Chapter 1: Project Setup**\n",
        "\n",
        "This chapter covers the initial setup, including importing necessary libraries and loading the datasets from their sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da82105f"
      },
      "source": [
        "### **1.1. Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2402d38"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import xgboost as xgb\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "429ab218"
      },
      "source": [
        "### **1.2. Load Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27b70eea"
      },
      "source": [
        "data1 = 'https://raw.githubusercontent.com/LatiefDataVisionary/demand-forecasting-retail-college-task/refs/heads/main/data/raw/Dataset%201_Retail%20Store%20Inventory%20and%20Demand%20Forecasting.csv'\n",
        "data2 = 'https://raw.githubusercontent.com/LatiefDataVisionary/demand-forecasting-retail-college-task/refs/heads/main/data/raw/Dataset%202_Retail%20Sales%20Data%20with%20Seasonal%20Trends%20%26%20Marketing.csv'\n",
        "data3 = 'https://raw.githubusercontent.com/LatiefDataVisionary/demand-forecasting-retail-college-task/refs/heads/main/data/raw/Dataset%203_Strategic%20Supply%20Chain%20Demand%20Forecasting%20Dataset.csv'\n",
        "\n",
        "df1 = pd.read_csv(data1)\n",
        "df2 = pd.read_csv(data2)\n",
        "df3 = pd.read_csv(data3)\n",
        "\n",
        "print(\"df1 loaded successfully.\")\n",
        "print(\"df2 loaded successfully.\")\n",
        "print(\"df3 loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.1. Data Information**"
      ],
      "metadata": {
        "id": "0uLpqcq0HTtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "libPz81qIAw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "GK-MS_zYIJOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "LVYh9mM9ID7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.info()"
      ],
      "metadata": {
        "id": "Il2YFOBtIK2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.head()"
      ],
      "metadata": {
        "id": "H255saBmIGsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.info()"
      ],
      "metadata": {
        "id": "Mb3FM4kDINlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.2. Data Summary Statistics**"
      ],
      "metadata": {
        "id": "Xa92FWqnHU37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.describe(include='all')"
      ],
      "metadata": {
        "id": "UGbSQgxWOqmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.describe(include='all')"
      ],
      "metadata": {
        "id": "6baw5ynUOxPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.describe(include='all')"
      ],
      "metadata": {
        "id": "37x5tfTbO0tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.3. Unique Values and Counts for Object Columns**"
      ],
      "metadata": {
        "id": "XpopLo0aHVRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df1.select_dtypes(include='object').columns:\n",
        "    print(f\"### Column: {col}\\n\")\n",
        "    print(f\"**Unique Values:**\\n{df1[col].unique()}\\n\")\n",
        "    print(f\"**Value Counts:**\\n{df1[col].value_counts().to_markdown(numalign='left', stralign='left')}\\n---\")"
      ],
      "metadata": {
        "id": "ae_2PcGtPzyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df2.select_dtypes(include='object').columns:\n",
        "    print(f\"### Column: {col}\\n\")\n",
        "    print(f\"**Unique Values:**\\n{df2[col].unique()}\\n\")\n",
        "    print(f\"**Value Counts:**\\n{df2[col].value_counts().to_markdown(numalign='left', stralign='left')}\\n---\")"
      ],
      "metadata": {
        "id": "Hmr6jGJrP-i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df3.select_dtypes(include='object').columns:\n",
        "    print(f\"### Column: {col}\\n\")\n",
        "    print(f\"**Unique Values:**\\n{df3[col].unique()}\\n\")\n",
        "    print(f\"**Value Counts:**\\n{df3[col].value_counts().to_markdown(numalign='left', stralign='left')}\\n---\")"
      ],
      "metadata": {
        "id": "dC_1UmExP-ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.4. Missing Values Analysis**"
      ],
      "metadata": {
        "id": "fGaAW3jDHVpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing values in df:\")\n",
        "print(df1.isnull().sum().to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "print(\"\\nDuplicate rows in df1:\")\n",
        "print(df1.duplicated().sum())"
      ],
      "metadata": {
        "id": "a0MBytPNQvvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing values in df2:\")\n",
        "print(df2.isnull().sum().to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "print(\"\\nDuplicate rows in df2:\")\n",
        "print(df2.duplicated().sum())"
      ],
      "metadata": {
        "id": "ZPEO0I7OQvr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing values in df3:\")\n",
        "print(df3.isnull().sum().to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "print(\"\\nDuplicate rows in df3:\")\n",
        "print(df3.duplicated().sum())"
      ],
      "metadata": {
        "id": "kb4iyTlMQvoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.5. Duplicate Values Analysis**"
      ],
      "metadata": {
        "id": "h9hHmRCtHV8d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c8a79fb"
      },
      "source": [
        "## **Chapter 2: Data Ingestion and Merging**\n",
        "\n",
        "The first step in our pipeline is to integrate the three disparate datasets into a single, unified master DataFrame that will serve as the foundation for our analysis and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dcbc6cc"
      },
      "source": [
        "### **2.1. Prepare Datasets for Merging**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88bcaff9"
      },
      "source": [
        "Standardize column names for consistency before merging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed176a8e"
      },
      "source": [
        "# Rename columns in df2 and df3\n",
        "df2 = df2.rename(columns={'ITEM CODE': 'Product ID'})\n",
        "df3 = df3.rename(columns={'date': 'Date', 'product_id': 'Product ID'})\n",
        "\n",
        "# Convert Product ID in df2 to string to match df1 and df3\n",
        "df2['Product ID'] = df2['Product ID'].astype(str)\n",
        "df1['Product ID'] = df1['Product ID'].astype(str)\n",
        "df3['Product ID'] = df3['Product ID'].astype(str)\n",
        "\n",
        "\n",
        "print(\"Column names standardized and Product ID data types converted.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c1192ee"
      },
      "source": [
        "Kode di bagian ini bertujuan untuk mempersiapkan tiga dataset (`df1`, `df2`, dan `df3`) sebelum digabungkan. Langkah-langkah yang dilakukan adalah:\n",
        "\n",
        "1.  **Menyeragamkan Nama Kolom:** Mengganti nama kolom di `df2` dan `df3` agar sesuai dengan nama kolom di `df1` yang akan digunakan sebagai kunci penggabungan (`merge`). Kolom 'ITEM CODE' di `df2` dan 'product_id' di `df3` diganti menjadi 'Product ID'. Kolom 'date' di `df3` diganti menjadi 'Date'.\n",
        "2.  **Mengubah Tipe Data Kolom 'Product ID':** Memastikan bahwa kolom 'Product ID' di ketiga dataset memiliki tipe data yang sama, yaitu string. Ini penting karena penggabungan berdasarkan kolom ini memerlukan tipe data yang konsisten. Kode ini secara eksplisit mengubah tipe data kolom 'Product ID' di `df1`, `df2`, dan `df3` menjadi string.\n",
        "\n",
        "Dengan menyeragamkan nama kolom dan tipe data 'Product ID', dataset siap untuk digabungkan menjadi satu DataFrame utama."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58479dd1"
      },
      "source": [
        "### **2.2. Merge Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f01908ea"
      },
      "source": [
        "Perform a two-step merge process to create the `master_df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77d1f4a1"
      },
      "source": [
        "# Merge df1 with df2\n",
        "master_df = pd.merge(df1, df2[['Product ID', 'SUPPLIER', 'ITEM TYPE', 'RETAIL SALES']], on='Product ID', how='left')\n",
        "\n",
        "# Merge the result with df3\n",
        "master_df = pd.merge(master_df, df3[['Date', 'Product ID', 'holiday_season', 'promotion_applied',\n",
        "                                     'competitor_price_index', 'economic_index', 'weather_impact',\n",
        "                                     'price', 'discount_percentage', 'sales_revenue', 'region_Europe',\n",
        "                                     'region_North America', 'store_type_Retail', 'store_type_Wholesale',\n",
        "                                     'category_Cabinets', 'category_Chairs', 'category_Sofas',\n",
        "                                     'category_Tables', 'future_demand']],\n",
        "                     on=['Date', 'Product ID'], how='left')\n",
        "\n",
        "print(\"Datasets merged successfully.\")\n",
        "display(master_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d73d7b00"
      },
      "source": [
        "## **Chapter 3: The 10-Step Data Preparation Pipeline**\n",
        "\n",
        "This chapter details the comprehensive, 10-step data preparation and cleaning process required to transform the raw, merged data into a feature-rich, model-ready format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f15c32a"
      },
      "source": [
        "### **Step 1: Data Cleaning and Type Conversion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78e3e0e"
      },
      "source": [
        "Inspect the Master DataFrame and handle missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5c5d8d8"
      },
      "source": [
        "# Inspect the Master DataFrame\n",
        "print(\"Info of master_df:\")\n",
        "master_df.info()\n",
        "\n",
        "print(\"\\nHead of master_df:\")\n",
        "display(master_df.head())\n",
        "\n",
        "# Handle Missing Values\n",
        "print(\"\\nMissing values before handling:\")\n",
        "print(master_df.isnull().sum().to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Fill missing 'SUPPLIER' and 'ITEM TYPE' with \"Unknown\"\n",
        "master_df['SUPPLIER'] = master_df['SUPPLIER'].fillna('Unknown')\n",
        "master_df['ITEM TYPE'] = master_df['ITEM TYPE'].fillna('Unknown')\n",
        "\n",
        "\n",
        "# Fill missing numerical columns with 0, as the merge resulted in all NaNs for these columns\n",
        "numerical_cols_from_merge = ['RETAIL SALES', 'holiday_season', 'promotion_applied',\n",
        "                               'competitor_price_index', 'economic_index', 'weather_impact',\n",
        "                               'price', 'discount_percentage', 'sales_revenue']\n",
        "\n",
        "for col in numerical_cols_from_merge:\n",
        "    if col in master_df.columns and master_df[col].isnull().all(): # Check if all values are NaN\n",
        "        master_df[col] = master_df[col].fillna(0) # Fill with 0\n",
        "        print(f\"Filled all missing values in {col} with 0.\")\n",
        "    elif col in master_df.columns and master_df[col].isnull().any(): # Check if some values are NaN\n",
        "         median_val = master_df[col].median()\n",
        "         master_df[col] = master_df[col].fillna(median_val)\n",
        "         print(f\"Filled some missing values in {col} with median ({median_val}).\")\n",
        "\n",
        "\n",
        "# Fill missing boolean columns with False (assuming NaN in boolean columns implies the condition is false)\n",
        "boolean_cols_with_missing = ['region_Europe', 'region_North America', 'store_type_Retail', 'store_type_Wholesale',\n",
        "                             'category_Cabinets', 'category_Chairs', 'category_Sofas', 'category_Tables']\n",
        "for col in boolean_cols_with_missing:\n",
        "     if col in master_df.columns and master_df[col].isnull().any():\n",
        "         master_df[col] = master_df[col].fillna(False)\n",
        "         print(f\"Filled missing values in {col} with False.\")\n",
        "\n",
        "\n",
        "print(\"\\nMissing values after handling:\")\n",
        "print(master_df.isnull().sum().to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Convert Date column to datetime object\n",
        "master_df['Date'] = pd.to_datetime(master_df['Date'])\n",
        "print(\"\\n'Date' column converted to datetime.\")\n",
        "\n",
        "# Check for Duplicates\n",
        "print(\"\\nDuplicate rows in master_df:\")\n",
        "print(master_df.duplicated().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dc40ca9"
      },
      "source": [
        "### **Step 2: Column Consolidation and Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2078f90"
      },
      "source": [
        "Identify and drop redundant or irrelevant columns. Select the target variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "447f23c8"
      },
      "source": [
        "# Identify and Drop Redundant Columns\n",
        "# 'future_demand' is a data leak and must be dropped.\n",
        "# 'ITEM DESCRIPTION' might be redundant given 'Product ID'.\n",
        "columns_to_drop = ['future_demand']\n",
        "master_df = master_df.drop(columns=columns_to_drop)\n",
        "\n",
        "print(f\"Dropped redundant columns: {columns_to_drop}\")\n",
        "\n",
        "# Target Variable Selection\n",
        "target_variable = 'Demand'\n",
        "y = master_df[target_variable]\n",
        "X = master_df.drop(columns=[target_variable])\n",
        "\n",
        "print(f\"Target variable '{target_variable}' selected.\")\n",
        "print(\"Features DataFrame (X) created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "889f1082"
      },
      "source": [
        "### **Step 3: Exploratory Data Analysis (EDA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dec87f3d"
      },
      "source": [
        "Visualize key aspects of the data to understand trends, distributions, and relationships."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a83af13d"
      },
      "source": [
        "# Time Series Analysis: Plot daily average Demand\n",
        "daily_demand = master_df.groupby('Date')['Demand'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.lineplot(data=daily_demand, x='Date', y='Demand')\n",
        "plt.title('Daily Average Demand Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Demand')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Distribution Analysis\n",
        "# Histograms for numerical features\n",
        "# Define numerical features more explicitly\n",
        "numerical_features = ['Inventory Level', 'Units Sold', 'Units Ordered', 'Price', 'Discount',\n",
        "                      'Promotion', 'Competitor Pricing', 'Epidemic', 'RETAIL SALES',\n",
        "                      'holiday_season', 'promotion_applied', 'competitor_price_index',\n",
        "                      'economic_index', 'weather_impact', 'price', 'discount_percentage',\n",
        "                      'sales_revenue', 'Demand'] # Included 'Demand' for distribution\n",
        "\n",
        "\n",
        "master_df[numerical_features].hist(figsize=(15, 10), bins=30)\n",
        "plt.suptitle('Histograms of Numerical Features', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Count plots for categorical features\n",
        "# Define categorical features more explicitly\n",
        "categorical_features = ['Category', 'Region', 'Store ID', 'Weather Condition', 'Seasonality', 'SUPPLIER', 'ITEM TYPE'] # Added SUPPLIER and ITEM TYPE\n",
        "\n",
        "for col in categorical_features:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    # Use `dropna=False` to include potential NaN category from merge if any (though we expect them to be filled now)\n",
        "    sns.countplot(data=master_df, y=col, order=master_df[col].value_counts(dropna=False).index, palette='viridis')\n",
        "    plt.title(f'Count Plot of {col}')\n",
        "    plt.xlabel('Count')\n",
        "    plt.ylabel(col)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Correlation Analysis for numerical features\n",
        "plt.figure(figsize=(12, 10))\n",
        "# Ensure only numerical columns are included for correlation calculation\n",
        "numerical_for_corr = master_df.select_dtypes(include=np.number).columns.tolist()\n",
        "correlation_matrix = master_df[numerical_for_corr].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72fc2023"
      },
      "source": [
        "### **Step 4: Outlier Detection and Treatment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1644cb77"
      },
      "source": [
        "# Visualize Outliers using Box Plots\n",
        "outlier_features = ['Units Sold', 'Price', 'Demand', 'Inventory Level', 'RETAIL SALES']\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "master_df[outlier_features].boxplot()\n",
        "plt.title('Box Plots for Outlier Detection')\n",
        "plt.ylabel('Value')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Treat Outliers (Optional but Recommended)\n",
        "# Explain the concept of capping outliers using the IQR method.\n",
        "print(\"Outlier Treatment (Capping using IQR method):\")\n",
        "print(\"The Interquartile Range (IQR) method can be used to identify outliers.\")\n",
        "print(\"Values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR can be considered outliers.\")\n",
        "print(\"Capping involves replacing these outliers with the boundary values (Q1 - 1.5*IQR or Q3 + 1.5*IQR).\")\n",
        "print(\"Below is commented-out code demonstrating how to cap outliers for 'Units Sold' as an example:\")\n",
        "\n",
        "# Example commented-out code for capping 'Units Sold' outliers:\n",
        "Q1 = master_df['Units Sold'].quantile(0.25)\n",
        "Q3 = master_df['Units Sold'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "master_df['Units Sold_capped'] = master_df['Units Sold'].clip(lower=lower_bound, upper=upper_bound)\n",
        "print(\"\\nExample: 'Units Sold' capped (new column 'Units Sold_capped' created).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dbbdfe7"
      },
      "source": [
        "Visualize potential outliers and provide an example of outlier treatment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3e58b02"
      },
      "source": [
        "### **Step 5: Feature Engineering I - Time-Based Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97e2740c"
      },
      "source": [
        "Create new features extracted from the 'Date' column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76cc87c2"
      },
      "source": [
        "# Create time-based features\n",
        "master_df['year'] = master_df['Date'].dt.year\n",
        "master_df['month'] = master_df['Date'].dt.month\n",
        "master_df['day'] = master_df['Date'].dt.day\n",
        "master_df['dayofweek'] = master_df['Date'].dt.dayofweek # Monday=0, Sunday=6\n",
        "master_df['dayofyear'] = master_df['Date'].dt.dayofyear\n",
        "master_df['weekofyear'] = master_df['Date'].dt.isocalendar().week.astype(int)\n",
        "master_df['quarter'] = master_df['Date'].dt.quarter\n",
        "\n",
        "print(\"Time-based features created.\")\n",
        "display(master_df[['Date', 'year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca10981c"
      },
      "source": [
        "### **Step 6: Feature Engineering II - Lag Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a27750de"
      },
      "source": [
        "Lag features are crucial for time-series forecasting as they represent past values of the target variable, capturing temporal dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70514cd5"
      },
      "source": [
        "# Create lag features for the 'Demand' column\n",
        "master_df['demand_lag_7'] = master_df.groupby('Product ID')['Demand'].shift(7)\n",
        "master_df['demand_lag_28'] = master_df.groupby('Product ID')['Demand'].shift(28)\n",
        "\n",
        "# Handle resulting NaN values (e.g., fill with 0 or the mean of the lag feature)\n",
        "# Filling with 0 as these are the initial periods where no lag data is available\n",
        "master_df['demand_lag_7'] = master_df['demand_lag_7'].fillna(0)\n",
        "master_df['demand_lag_28'] = master_df['demand_lag_28'].fillna(0)\n",
        "\n",
        "\n",
        "print(\"Lag features created for 'Demand'.\")\n",
        "display(master_df[['Date', 'Product ID', 'Demand', 'demand_lag_7', 'demand_lag_28']].head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b9e34cc"
      },
      "source": [
        "### **Step 7: Feature Engineering III - Rolling Window Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b40be5c"
      },
      "source": [
        "Rolling window features, such as rolling means, help smooth out noise and capture short-term trends in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9ac0c27"
      },
      "source": [
        "# Create rolling mean features for 'Demand'\n",
        "master_df['demand_rolling_mean_7'] = master_df.groupby('Product ID')['Demand'].transform(lambda x: x.rolling(window=7).mean())\n",
        "master_df['demand_rolling_mean_28'] = master_df.groupby('Product ID')['Demand'].transform(lambda x: x.rolling(window=28).mean())\n",
        "\n",
        "# Handle resulting NaN values (initial periods) by filling with 0\n",
        "# Filling with 0 as these are the initial periods where no rolling data is available\n",
        "master_df['demand_rolling_mean_7'] = master_df['demand_rolling_mean_7'].fillna(0)\n",
        "master_df['demand_rolling_mean_28'] = master_df['demand_rolling_mean_28'].fillna(0)\n",
        "\n",
        "\n",
        "print(\"Rolling window features created for 'Demand'.\")\n",
        "display(master_df[['Date', 'Product ID', 'Demand', 'demand_rolling_mean_7', 'demand_rolling_mean_28']].head(30)) # Display more rows to see non-NaN values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17da0191"
      },
      "source": [
        "### **Step 8: Categorical Feature Encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54cfe6f4"
      },
      "source": [
        "Convert categorical features into a numerical format using One-Hot Encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb2fe3bf"
      },
      "source": [
        "# Identify categorical columns (excluding 'Date' and 'Product ID' which are not features for encoding here)\n",
        "categorical_features_for_encoding = master_df.select_dtypes(include='object').columns.tolist()\n",
        "# Include 'Store ID' for one-hot encoding\n",
        "# Note: 'Category', 'Region', 'Weather Condition', 'Seasonality', 'SUPPLIER', 'ITEM TYPE' are good candidates\n",
        "\n",
        "print(f\"Categorical features to encode: {categorical_features_for_encoding}\")\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "master_df = pd.get_dummies(master_df, columns=categorical_features_for_encoding, dummy_na=False)\n",
        "\n",
        "print(\"Categorical features encoded using One-Hot Encoding.\")\n",
        "display(master_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c8ec295"
      },
      "source": [
        "### **Step 9: Feature Scaling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26b22b11"
      },
      "source": [
        "Scale numerical features to standardize their range, which is important for many machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cefdf526"
      },
      "source": [
        "# Identify numerical features to scale\n",
        "# Exclude 'Date', 'Product ID', the target variable 'Demand', and the newly created binary/boolean columns from one-hot encoding\n",
        "numerical_features_to_scale = X.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Remove boolean columns resulting from one-hot encoding if they are still in X\n",
        "boolean_cols = X.select_dtypes(include='bool').columns.tolist()\n",
        "numerical_features_to_scale = [col for col in numerical_features_to_scale if col not in boolean_cols]\n",
        "\n",
        "# Exclude lag and rolling features as they are already derived from scaled data or handled\n",
        "lag_rolling_features = ['demand_lag_7', 'demand_lag_28', 'demand_rolling_mean_7', 'demand_rolling_mean_28', 'Units Sold_capped'] # Added 'Units Sold_capped'\n",
        "numerical_features_to_scale = [col for col in numerical_features_to_scale if col not in lag_rolling_features]\n",
        "\n",
        "print(f\"Numerical features to scale: {numerical_features_to_scale}\")\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "master_df[numerical_features_to_scale] = scaler.fit_transform(master_df[numerical_features_to_scale])\n",
        "\n",
        "print(\"Numerical features scaled using StandardScaler.\")\n",
        "display(master_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40d000b3"
      },
      "source": [
        "### **Step 10: Time-Based Data Splitting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17f5a195"
      },
      "source": [
        "For time-series data, it is critical to split the data chronologically to avoid data leakage from the future into the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69cd5e81"
      },
      "source": [
        "# Define the chronological cutoff date for splitting\n",
        "# Using a date in early 2023 as an example cutoff\n",
        "cutoff_date = pd.to_datetime('2023-01-01')\n",
        "\n",
        "# Split data based on the cutoff date\n",
        "train_df = master_df[master_df['Date'] < cutoff_date].copy()\n",
        "val_df = master_df[master_df['Date'] >= cutoff_date].copy()\n",
        "\n",
        "# Define features (X) and target (y) for train and validation sets\n",
        "# Exclude 'Date' and 'Product ID' as they are not features for the model\n",
        "features = [col for col in master_df.columns if col not in ['Date', 'Product ID', target_variable]]\n",
        "\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_variable]\n",
        "\n",
        "X_val = val_df[features]\n",
        "y_val = val_df[target_variable]\n",
        "\n",
        "\n",
        "print(f\"Data split into training and validation sets based on cutoff date: {cutoff_date}\")\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Validation set shape: {X_val.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ad9307f"
      },
      "source": [
        "## **Chapter 4: Model Training and Evaluation**\n",
        "\n",
        "This chapter focuses on selecting, training, and evaluating a machine learning model for demand forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d75afd4a"
      },
      "source": [
        "### **4.1. Model Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ef4a40"
      },
      "source": [
        "We will use XGBoost (Extreme Gradient Boosting) for our demand forecasting model due to its strong performance on structured data and its ability to handle various types of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ebc231f"
      },
      "source": [
        "### **4.2. Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6168b3c"
      },
      "source": [
        "# Initialize and train the XGBoost Regressor model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', # Regression task\n",
        "                             n_estimators=1000,          # Number of boosting rounds\n",
        "                             learning_rate=0.05,         # Step size shrinkage\n",
        "                             max_depth=7,                # Maximum depth of a tree\n",
        "                             random_state=42,            # For reproducibility\n",
        "                             n_jobs=-1)                  # Use all available cores\n",
        "\n",
        "print(\"Training XGBoost model...\")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "print(\"XGBoost model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c482847"
      },
      "source": [
        "### **4.3. Model Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fb55ac"
      },
      "source": [
        "Evaluate the trained model using appropriate regression metrics on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07de8ae7"
      },
      "source": [
        "# Make predictions on the validation set\n",
        "y_pred = xgb_model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "print(f\"Model Evaluation on Validation Set:\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96678b2d"
      },
      "source": [
        "### **4.4. Feature Importance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da1e8d1e"
      },
      "source": [
        "Analyze the importance of different features in the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dc53237"
      },
      "source": [
        "# Get feature importances from the trained model\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame of feature importances\n",
        "feature_importance_df = pd.DataFrame({'feature': X_train.columns, 'importance': feature_importances})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Display top N features\n",
        "print(\"\\nTop 15 Feature Importances:\")\n",
        "display(feature_importance_df.head(15))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance_df.head(15))\n",
        "plt.title('Top 15 Feature Importances')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "115bbbd0"
      },
      "source": [
        "## **Chapter 5: Model Evaluation**\n",
        "\n",
        "In this chapter, we evaluate the performance of our trained models on the unseen validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fd313e4"
      },
      "source": [
        "### **5.1. Make Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41070cdf"
      },
      "source": [
        "# Make predictions with all three models on X_val\n",
        "y_pred_xgb = xgb_model.predict(X_val)\n",
        "# Assuming LinearRegression and RandomForestRegressor models were trained in Chapter 4\n",
        "# If not, you would need to train them here first\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred_lr = lr_model.predict(X_val)\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_val)\n",
        "\n",
        "print(\"Predictions made with XGBoost model.\")\n",
        "print(\"Predictions made with Linear Regression model.\")\n",
        "print(\"Predictions made with Random Forest Regressor model.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf2b4a4f"
      },
      "source": [
        "### **5.2. Compare Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2191bcd7"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Calculate RMSE and MAE for each model\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n",
        "mae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n",
        "\n",
        "# # Assuming y_pred_lr and y_pred_rf are available from the previous cell\n",
        "rmse_lr = np.sqrt(mean_squared_error(y_val, y_pred_lr))\n",
        "mae_lr = mean_absolute_error(y_val, y_pred_lr)\n",
        "\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_val, y_pred_rf))\n",
        "mae_rf = mean_absolute_error(y_val, y_pred_rf)\n",
        "\n",
        "\n",
        "# Present the results clearly in a pandas DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['XGBoost', 'Linear Regression', 'Random Forest'], # Add 'Linear Regression', 'Random Forest' if those models were trained\n",
        "    'RMSE': [rmse_xgb, rmse_lr, rmse_rf], # Add rmse_lr, rmse_rf\n",
        "    'MAE': [mae_xgb, mae_lr, mae_rf] # Add mae_lr, mae_rf\n",
        "})\n",
        "\n",
        "# results_df = pd.DataFrame({\n",
        "#     'Model': ['XGBoost', 'Random Forest'], # Add 'Linear Regression', 'Random Forest' if those models were trained\n",
        "#     'RMSE': [rmse_xgb, rmse_rf], # Add rmse_lr, rmse_rf\n",
        "#     'MAE': [mae_xgb, mae_rf] # Add mae_lr, mae_rf\n",
        "# })\n",
        "\n",
        "print(\"Model Performance Comparison:\")\n",
        "display(results_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "981bb893"
      },
      "source": [
        "## **Chapter 6: Analysis of the Best Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d40ea08"
      },
      "source": [
        "Based on the evaluation metrics, we select the best-performing model and conduct a deeper analysis. Since XGBoost generally performs well, we will focus on its analysis here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "254be1fd"
      },
      "source": [
        "### **6.1. Visualize Predictions vs. Actuals**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d91114c7"
      },
      "source": [
        "# Create a DataFrame for plotting\n",
        "plot_df = pd.DataFrame({'Actual': y_val, 'Predicted': y_pred_xgb}, index=X_val.index)\n",
        "\n",
        "# Sort by Date for a meaningful time series plot\n",
        "plot_df = plot_df.sort_index()\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(val_df['Date'], plot_df['Actual'], label='Actual Demand', alpha=0.7)\n",
        "plt.plot(val_df['Date'], plot_df['Predicted'], label='Predicted Demand', alpha=0.7)\n",
        "plt.title('XGBoost Predictions vs. Actuals Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Demand')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f900a44"
      },
      "source": [
        "###**6.2. Feature Importance Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29187c61"
      },
      "source": [
        "# Get feature importances from the trained XGBoost model\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame of feature importances\n",
        "feature_importance_df = pd.DataFrame({'feature': X_train.columns, 'importance': feature_importances})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Display top 20 features\n",
        "print(\"\\nTop 20 Feature Importances (XGBoost):\")\n",
        "display(feature_importance_df.head(20))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance_df.head(20))\n",
        "plt.title('Top 20 Feature Importances (XGBoost)')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7c6dd6a"
      },
      "source": [
        "## **Chapter 7: Conclusion and Recommendations**\n",
        "\n",
        "This chapter summarizes the project's findings, provides actionable business recommendations based on the model's insights, and outlines potential areas for future work to further improve demand forecasting accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1d95320"
      },
      "source": [
        "### **7.1. Final Summary**\n",
        "\n",
        "The objective of this project was to develop a machine learning pipeline for daily product demand forecasting using multiple retail datasets. The pipeline involved comprehensive data ingestion, merging, and a 10-step data preparation process including feature engineering and scaling.\n",
        "\n",
        "Three regression models were trained and evaluated on a chronologically split validation set: XGBoost, Linear Regression, and Random Forest.\n",
        "\n",
        "Based on the evaluation metrics on the validation set:\n",
        "\n",
        "*   **XGBoost Model:**\n",
        "    *   Root Mean Squared Error (RMSE): 13.49\n",
        "    *   Mean Absolute Error (MAE): 9.99\n",
        "\n",
        "*   **Linear Regression Model:**\n",
        "    *   Root Mean Squared Error (RMSE): 21.15\n",
        "    *   Mean Absolute Error (MAE): 15.77\n",
        "\n",
        "*   **Random Forest Model:**\n",
        "    *   Root Mean Squared Error (RMSE): 17.08\n",
        "    *   Mean Absolute Error (MAE): 12.85\n",
        "\n",
        "The **XGBoost model** demonstrated the best performance, achieving the lowest RMSE and MAE scores on the validation set. This indicates that the XGBoost model is the most accurate among the tested models for forecasting daily product demand in this context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0a15d2c"
      },
      "source": [
        "### **7.2. Business Recommendations**\n",
        "\n",
        "The feature importance analysis of the best-performing XGBoost model provides valuable insights for business strategy and inventory management:\n",
        "\n",
        "*   **Dominance of Sales and Inventory Metrics:** Features directly related to recent sales activity (`Units Sold`, `Units Sold_capped`, `Units Ordered`) and current stock levels (`Inventory Level`) are the most significant drivers of forecasted demand. This highlights the strong influence of immediate historical performance and stock availability on near-future demand.\n",
        "    *   **Recommendation:** Maintain accurate, real-time tracking of units sold, units ordered, and inventory levels. These metrics are critical inputs for effective forecasting and should be prioritized for data quality. Rapid response strategies based on recent sales trends are likely to be effective.\n",
        "\n",
        "*   **Impact of Promotions:** `Promotion` is identified as a significant feature. This confirms that promotional activities have a direct and substantial impact on increasing demand.\n",
        "    *   **Recommendation:** Continue to leverage strategic promotions to stimulate demand. Analyze the effectiveness of different types and timings of promotions based on their correlation with actual sales outcomes. The model can potentially be used to forecast the impact of planned promotions.\n",
        "\n",
        "*   **Influence of Lagged and Rolling Demand:** Lag features (`demand_lag_7`) and rolling window features (`demand_rolling_mean_7`) are important predictors. This underscores the temporal dependency of demand â€“ past demand patterns are strong indicators of future demand.\n",
        "    *   **Recommendation:** Incorporate recent historical demand trends into planning. Be mindful of weekly seasonality (captured by `demand_lag_7`) and short-term demand fluctuations (captured by `demand_rolling_mean_7`) when making inventory decisions.\n",
        "\n",
        "*   **Category and Product Specificity:** Specific product categories (`Category_Furniture`, `Category_Groceries`, `Category_Clothing`, `Category_Electronics`, `Category_Toys`) and even individual products (`Product ID_P0013`, `Product ID_P0002`, etc.) show notable importance. Demand patterns vary significantly by product type.\n",
        "    *   **Recommendation:** Develop category and product-specific forecasting strategies where appropriate. Tailor inventory management and marketing efforts based on the distinct demand characteristics of different product groups.\n",
        "\n",
        "*   **Environmental and External Factors:** Features like `Weather Condition_Sunny` and `Epidemic` also contribute to demand prediction.\n",
        "    *   **Recommendation:** Monitor external factors such as weather forecasts and public health situations, as they can influence consumer behavior and demand. Incorporate these insights into dynamic adjustments of forecasts and inventory.\n",
        "\n",
        "By focusing on these key drivers identified by the model, the retail company can make more informed decisions regarding inventory, marketing, and supply chain operations, ultimately leading to improved efficiency and reduced costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "797723f0"
      },
      "source": [
        "### **7.3. Future Work**\n",
        "\n",
        "To further enhance the demand forecasting model and pipeline, the following areas are recommended for future work:\n",
        "\n",
        "*   **Hyperparameter Tuning:** Systematically tune the hyperparameters of the XGBoost model (and potentially other models) using techniques like Grid Search or Randomized Search with cross-validation to find the optimal configuration for better performance.\n",
        "*   **Explore More Advanced Models:** Investigate other time-series forecasting models, such as ARIMA, Prophet, or deep learning models like LSTMs or GRUs, which can be particularly effective in capturing complex temporal patterns.\n",
        "*   **Additional Feature Engineering:**\n",
        "    *   Create more sophisticated lag and rolling window features (e.g., different window sizes, standard deviations, minimums, maximums).\n",
        "    *   Incorporate calendar features (e.g., is\\_weekend, is\\_holiday, days until next holiday).\n",
        "    *   Engineer interaction features between product/store/category and time-based features.\n",
        "*   **Incorporate External Data:** If available, integrate additional external data sources that could influence demand, such as:\n",
        "    *   Local events or promotions data.\n",
        "    *   Economic indicators specific to the region or market.\n",
        "    *   Social media trends or search interest data related to products.\n",
        "*   **Cross-Validation Strategy:** Implement a more robust time-series cross-validation strategy (e.g., rolling origin validation) during model training and evaluation to get a more reliable estimate of model performance on unseen future data.\n",
        "*   **Anomaly Detection:** Implement anomaly detection techniques to identify and potentially treat unusual spikes or drops in demand that might skew the model.\n",
        "*   **Model Interpretability:** While XGBoost provides feature importance, explore other methods for model interpretability (e.g., SHAP values) to gain a deeper understanding of how individual features influence specific predictions.\n",
        "*   **Deployment:** Develop a strategy for deploying the trained model to make real-time or near real-time demand forecasts and integrate it into the company's inventory management and planning systems."
      ]
    }
  ]
}