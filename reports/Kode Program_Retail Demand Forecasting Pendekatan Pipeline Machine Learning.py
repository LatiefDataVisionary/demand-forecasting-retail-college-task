# -*- coding: utf-8 -*-
"""complete_tpm_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/LatiefDataVisionary/demand-forecasting-retail-college-task/blob/main/notebooks/complete_tpm_notebook.ipynb

# **Demand Forecasting for Retail: A Machine Learning Pipeline Approach**

## **Project Overview**

**College Course:** Model Development Engineering

**Objective:** To develop a robust machine learning model to accurately forecast daily product demand for a retail company. This involves a comprehensive data pipeline, including merging multiple data sources, extensive feature engineering, and model comparison to optimize inventory management and reduce costs.

## **Tentang Dataset**

Fondasi dari proyek pemodelan prediktif ini adalah integrasi strategis dari tiga dataset yang berbeda namun saling melengkapi. Dengan menggabungkan sumber-sumber ini, kami bertujuan untuk menciptakan pandangan holistik tentang lingkungan ritel, menangkap segalanya mulai dari transaksi harian yang terperinci dan tingkat inventaris hingga upaya pemasaran yang lebih luas dan sinyal ekonomi makro. Pendekatan komprehensif ini sangat penting untuk membangun model peramalan permintaan yang kuat dan akurat.

Setiap dataset memainkan peran spesifik dan krusial dalam memperkaya dataset master akhir kami:


### **1. Retail Store Inventory and Demand Forecasting (Data Transaksional Inti)**

**Tautan:** [Kaggle Dataset](https://www.kaggle.com/datasets/atomicd/retail-store-inventory-and-demand-forecasting)

Dataset ini berfungsi sebagai **tabel dasar atau utama** untuk proyek kami. Ini menyediakan data transaksional dan operasional inti pada tingkat paling granular: aktivitas penjualan harian untuk setiap produk di setiap toko. Ini berisi sinyal utama perilaku pelanggan dan, yang paling penting, variabel target kami untuk prediksi.

**Peran dalam Proyek:**


Ini adalah sumber kebenaran utama kami untuk penjualan dan permintaan. Kami akan menggunakannya sebagai tabel utama tempat semua informasi lain akan digabungkan. Catatan hariannya yang terperinci sangat cocok untuk rekayasa fitur deret waktu (misalnya, membuat lag dan rata-rata bergulir).

**Fitur Utama yang Dimanfaatkan:**
- `Date`, `Store ID`, `Product ID`:

    Kunci utama yang digunakan untuk menggabungkan dan melacak kinerja di berbagai segmen.
- **`Demand` (Variabel Target):**

    Perkiraan permintaan harian untuk setiap produk, yang ingin diprediksi oleh model kami.
- `Units Sold`, `Inventory Level`:

    Indikator internal kritis kinerja stok dan kecepatan penjualan.
- `Price`, `Discount`, `Promotion`:

    Tuas keuangan utama yang secara langsung memengaruhi keputusan pembelian pelanggan.
- `Weather Condition`, `Seasonality`:

    Faktor eksternal yang membantu model memahami pola penjualan kontekstual.
- `Competitor Pricing`:

    Memberikan gambaran tentang lanskap kompetitif.
- `Epidemic`:
    Bendera biner unik untuk membantu model belajar dari periode gangguan pasar yang signifikan, seperti pandemi.

**Alasan Pemilihan Strategis:**

*   **Kelengkapan Data Granular:**

    Menyediakan data pada level paling detail yang kami butuhkan: penjualan **harian** untuk setiap **produk** di setiap **toko**. Granularitas ini sangat penting untuk analisis deret waktu (*time-series*) dan rekayasa fitur seperti *lag* dan *rolling window*.
*   **Mengandung Variabel Target Fundamental:**

    Secara eksplisit menyediakan kolom **`Demand`** dan **`Units Sold`**, yang merupakan variabel target yang akan kami prediksi. Ini adalah alasan utama dataset ini menjadi fondasi proyek.
*   **Kaya akan Fitur Internal yang Dapat Ditindaklanjuti:**

    Berisi fitur-fitur operasional inti yang secara langsung berada dalam kendali bisnis, seperti `Inventory Level`, `Price`, `Discount`, dan `Promotion`. Fitur-fitur ini adalah tuas utama yang digunakan manajemen ritel untuk mempengaruhi penjualan secara langsung.
*   **Konteks Tambahan yang Unik:**

    Adanya fitur seperti `Weather Condition` dan terutama **`Epidemic`** memberikan nilai tambah yang luar biasa. Fitur `Epidemic` memungkinkan model untuk belajar dari anomali atau periode disrupsi besar, membuatnya lebih tangguh (*robust*) terhadap kejadian tak terduga di masa depan.


### **2. Retail Sales Data with Seasonal Trends & Marketing (Data Konteks Pemasaran)**

**Tautan:** [Kaggle Dataset](https://www.kaggle.com/datasets/abdullah0a/retail-sales-data-with-seasonal-trends-and-marketing)

Sementara dataset pertama kami menyediakan detail operasional, dataset kedua ini menawarkan pandangan tingkat yang lebih tinggi tentang lingkungan pemasaran dan penjualan. Kontribusi utamanya adalah penyertaan pengeluaran pemasaran, memungkinkan model kami untuk mengukur dampak upaya periklanan terhadap penjualan.

**Peran dalam Proyek:**

Untuk memperkaya dataset master kami dengan intelijen pemasaran yang krusial. Dengan menggabungkan data ini, kami dapat menganalisis laba atas investasi untuk kampanye pemasaran dan memahami bagaimana pengeluaran pemasaran, bersama dengan faktor-faktor lain seperti hari libur dan diskon, mendorong permintaan.

**Fitur Utama yang Dimanfaatkan:**
- **`Marketing Spend (USD)`:**

    Fitur paling berharga dari dataset ini, memberikan ukuran langsung dari investasi pemasaran.
- `Sales Revenue (USD)`:

    Dapat digunakan untuk validasi atau sebagai fitur tambahan untuk menangkap nilai moneter penjualan.
- `Holiday Effect`:

    Indikator biner yang berguna untuk membantu model memahami peningkatan penjualan selama periode liburan.
- `Store Location`, `Product Category`:

    Memberikan dimensi tambahan untuk segmentasi dan analisis.
**Alasan Pemilihan Strategis:**

*   **Mengukur Dampak Pemasaran secara Kuantitatif:**

    Fitur utamanya adalah **`Marketing Spend (USD)`**. Ini adalah metrik yang sangat berharga yang tidak ada di dataset lain. Ini mengubah informasi `Promotion` (yang hanya berupa 'ya' atau 'tidak' di Dataset 1) menjadi metrik kuantitatif. Dengan ini, model dapat belajar hubungan antara **jumlah uang yang dihabiskan** untuk pemasaran dengan peningkatan penjualan, sehingga memungkinkan analisis ROI (*Return on Investment*).
*   **Memperkuat Analisis Musiman:**

    Adanya kolom `Holiday Effect` melengkapi kolom `Seasonality` dari Dataset 1. Ini memberikan sinyal yang lebih spesifik tentang lonjakan penjualan selama periode liburan tertentu, bukan hanya musim secara umum.
*   **Menyediakan Dimensi Keuangan Tambahan:**

    Kolom `Sales Revenue` memungkinkan analisis tidak hanya dari segi jumlah unit, tetapi juga dari nilai moneter, yang dapat digunakan sebagai fitur tambahan untuk memberikan bobot lebih pada penjualan bernilai tinggi atau untuk validasi model.

### **3. Strategic Supply Chain Demand Forecasting Dataset (Data Ekonomi & Kompetitif Eksternal)**

**Tautan:** [Kaggle Dataset](https://www.kaggle.com/datasets/ziya07/strategic-supply-chain-demand-forecasting-dataset)

Dataset terakhir ini menyediakan lapisan konteks ekonomi makro dan kompetitif. Ini mensimulasikan data dari perusahaan furnitur dan menyertakan indeks berharga yang mewakili faktor-faktor di luar kendali langsung toko ritel tunggal, seperti kesehatan ekonomi secara keseluruhan.

**Peran dalam Proyek:**

Untuk memberikan model kami sinyal eksternal yang memengaruhi daya beli dan perilaku konsumen secara keseluruhan. Fitur-fitur ini membantu model memahami kondisi pasar yang lebih luas, membuatnya lebih tangguh dan mudah beradaptasi terhadap pergeseran ekonomi. Dataset ini sangat berharga karena berisi fitur-fitur yang telah diproses sebelumnya dan siap model.

**Fitur Utama yang Dimanfaatkan:**
- **`economic_index`:**

    Fitur kuat yang mewakili kekuatan ekonomi. Indeks yang lebih tinggi menunjukkan kondisi ekonomi yang lebih kuat, yang biasanya berkorelasi dengan penjualan ritel yang lebih tinggi.
- **`competitor_price_index`:**

    Indeks yang mewakili harga relatif pesaing, memberikan pandangan yang lebih abstrak tentang lingkungan kompetitif daripada harga pesaing tunggal.
- Bendera Boolean yang Telah Diproses Sebelumnya:

    Fitur seperti `region_Europe`, `store_type_Retail`, dan `category_Chairs` menyediakan atribut yang berguna dan sudah dikodekan.

**Alasan Pemilihan Strategis:**

*   **Memberikan Sinyal Ekonomi Makro:**

    Fitur **`economic_index`** adalah alasan terkuat untuk memilih dataset ini. Penjualan ritel sangat berkorelasi dengan kesehatan ekonomi. Dengan fitur ini, model dapat beradaptasi dengan kondisi ekonomi yang sedang baik (booming) atau buruk (resesi), menjadikannya lebih cerdas dan proaktif, bukan hanya reaktif terhadap data masa lalu.
*   **Memberikan Gambaran Kompetisi yang Lebih Luas:**
    `competitor_price_index` memberikan pandangan yang lebih terabstraksi tentang lanskap persaingan. Sebuah indeks yang mewakili rata-rata pasar lebih stabil dan seringkali lebih informatif daripada hanya membandingkan dengan harga satu pesaing (`Competitor Pricing` di Dataset 1).
*   **Efisiensi dengan Fitur yang Sudah Terproses:**

    Dataset ini menyediakan fitur yang sudah di-*engineer* dan di-*encode* (seperti `region_Europe`, `store_type_Retail`), yang memungkinkan kami menunjukkan kemampuan untuk bekerja dengan berbagai format data dan menghemat waktu dalam preprocessing.


**Catatan Penting:**

    
Dataset ini berisi kolom bernama `future_demand`, yang tampaknya merupakan variabel target untuk kasus penggunaan aslinya. Untuk mencegah **kebocoran data (data leakage)**, kolom ini akan dihapus dari `master_df` kami. Tujuan proyek kami adalah untuk memprediksi variabel `Demand` dari dataset pertama.

Dengan menggabungkan ketiga sumber ini, kami membangun DataFrame master tunggal yang komprehensif. Dataset terpadu ini memungkinkan model kami untuk belajar dari berbagai sinyal—mulai dari tingkat stok internal dan promosi hingga penetapan harga pesaing dan tren ekonomi makro—membuka jalan bagi solusi peramalan yang lebih akurat dan kuat.

## **Chapter 1: Project Setup (Pengaturan Proyek)**

Bab ini mencakup pengaturan awal, termasuk mengimpor library yang diperlukan dan memuat dataset dari sumbernya.

### **1.1. Import Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error
import xgboost as xgb

# Set plotting style
sns.set_style("whitegrid")

"""Kode di atas mengimpor library Python yang akan digunakan sepanjang proyek ini:

*   **`pandas`**: Library utama untuk manipulasi dan analisis data, terutama untuk bekerja dengan DataFrame. Diimpor dengan alias `pd`.
*   **`numpy`**: Library untuk komputasi numerik, sangat berguna untuk operasi array dan matematika. Diimpor dengan alias `np`.
*   **`matplotlib.pyplot`**: Library untuk membuat visualisasi statis di Python. Diimpor dengan alias `plt`.
*   **`seaborn`**: Library visualisasi data tingkat tinggi berdasarkan matplotlib, menyediakan antarmuka yang lebih menarik dan informatif. Diimpor dengan alias `sns`.
*   **`sklearn.model_selection.train_test_split`**: Fungsi untuk membagi dataset menjadi subset pelatihan dan pengujian.
*   **`sklearn.preprocessing.StandardScaler`**: Digunakan untuk menstandarisasi fitur dengan menghapus rata-rata dan menskalakan ke varian unit.
*   **`sklearn.preprocessing.OneHotEncoder`**: Digunakan untuk mengonversi fitur kategorikal menjadi format one-hot numerik.
*   **`sklearn.compose.ColumnTransformer`**: Memungkinkan penerapan transformer yang berbeda ke kolom data yang berbeda.
*   **`sklearn.pipeline.Pipeline`**: Memungkinkan pembuatan pipeline kerja yang menggabungkan beberapa langkah preprocessing dan estimasi.
*   **`sklearn.metrics.mean_squared_error`**, **`sklearn.metrics.mean_absolute_error`**: Metrik evaluasi untuk model regresi.
*   **`xgboost`**: Library untuk implementasi Extreme Gradient Boosting (XGBoost), algoritma machine learning yang populer dan efisien. Diimpor dengan alias `xgb`.

Baris `sns.set_style("whitegrid")` mengatur gaya plot default untuk seaborn, memberikan latar belakang putih dengan grid.

### **1.2. Load Datasets**
"""

data1 = 'https://raw.githubusercontent.com/LatiefDataVisionary/demand-forecasting-retail-college-task/refs/heads/main/data/raw/Dataset%201_Retail%20Store%20Inventory%20and%20Demand%20Forecasting.csv'
data2 = 'https://raw.githubusercontent.com/LatiefDataVisionary/demand-forecasting-retail-college-task/refs/heads/main/data/raw/Dataset%202_Retail%20Sales%20Data%20with%20Seasonal%20Trends%20%26%20Marketing.csv'
data3 = 'https://raw.githubusercontent.com/LatiefDataVisionary/demand-forecasting-retail-college-task/refs/heads/main/data/raw/Dataset%203_Strategic%20Supply%20Chain%20Demand%20Forecasting%20Dataset.csv'

df1 = pd.read_csv(data1)
df2 = pd.read_csv(data2)
df3 = pd.read_csv(data3)

print("df1 loaded successfully.")
print("df2 loaded successfully.")
print("df3 loaded successfully.")

"""Kode di bagian ini bertujuan untuk memuat tiga dataset yang akan digunakan dalam proyek ini dari sumber URL mentah (raw).

1.  **Mendefinisikan URL Dataset**

    Tiga variabel string (`data1`, `data2`, `data3`) dibuat untuk menyimpan URL dari file CSV yang berada di repositori GitHub. Menggunakan URL mentah dari GitHub memungkinkan kode untuk langsung mengakses file data.
2.  **Memuat Dataset ke DataFrame:**

    Fungsi `pd.read_csv()` dari library pandas digunakan untuk membaca data dari setiap URL.
    *   `df1 = pd.read_csv(data1)`: Memuat data dari URL di `data1` ke dalam DataFrame bernama `df1`.
    *   `df2 = pd.read_csv(data2)`: Memuat data dari URL di `data2` ke dalam DataFrame bernama `df2`.
    *   `df3 = pd.read_csv(data3)`: Memuat data dari URL di `data3` ke dalam DataFrame bernama `df3`.
3.  **Konfirmasi Pemuatan:**

    Mencetak pesan konfirmasi untuk setiap dataset setelah berhasil dimuat. Ini membantu memverifikasi bahwa proses pemuatan berjalan tanpa kesalahan.

Setelah kode ini dijalankan, tiga DataFrame (`df1`, `df2`, dan `df3`) akan tersedia di lingkungan kerja, berisi data dari masing-masing file CSV.

#### **1.2.1. Data Information**
"""

df1.head()

df1.info()

df2.head()

df2.info()

df3.head()

df3.info()

"""Kode di bagian "1.2.1. Data Information" ini digunakan untuk mendapatkan gambaran awal tentang struktur dan isi dari setiap dataset yang telah dimuat (`df1`, `df2`, dan `df3`).

*   `df1.head()`, `df2.head()`, `df3.head()`:

    Perintah `.head()` menampilkan lima baris pertama dari setiap DataFrame. Ini berguna untuk melihat sekilas data, termasuk nama kolom, tipe data, dan beberapa nilai awal.
*   `df1.info()`, `df2.info()`, `df3.info()`:

    Perintah `.info()` mencetak ringkasan ringkas dari setiap DataFrame. Ini termasuk:
    *   Jumlah entri (baris) dalam DataFrame.
    *   Daftar semua kolom.
    *   Jumlah nilai non-null di setiap kolom, yang membantu mengidentifikasi kolom dengan nilai yang hilang.
    *   Tipe data (dtype) dari setiap kolom (misalnya, int64, float64, object).
    *   Penggunaan memori oleh DataFrame.

Dengan menjalankan kode ini, kita dapat dengan cepat memahami format data, mengidentifikasi potensi masalah seperti nilai yang hilang atau tipe data yang tidak sesuai, dan mempersiapkan langkah-langkah pemrosesan data selanjutnya.

#### **1.2.2. Data Summary Statistics**
"""

df1.describe(include='all')

df2.describe(include='all')

df3.describe(include='all')

"""Kode di bagian "1.2.2. Data Summary Statistics" ini digunakan untuk menghasilkan ringkasan statistik deskriptif untuk setiap kolom dalam DataFrame.

*   `df1.describe(include='all')`, `df2.describe(include='all')`, `df3.describe(include='all')`:

    Perintah `.describe()` menghasilkan statistik deskriptif. Dengan `include='all'`, ini mencakup kolom bertipe objek (non-numerik) serta kolom numerik. Untuk kolom numerik, output mencakup hitungan (count), rata-rata (mean), standar deviasi (std), nilai minimum (min), kuartil (25%, 50%, 75%), dan nilai maksimum (max). Untuk kolom objek, ini mencakup hitungan, jumlah nilai unik (unique), nilai yang paling sering muncul (top), dan frekuensinya (freq).

Ringkasan statistik ini membantu kita memahami distribusi data, rentang nilai, dan mengidentifikasi potensi masalah seperti pencilan (outliers) atau data yang tidak wajar dalam kolom numerik, serta memahami keragaman dan distribusi nilai dalam kolom kategorikal.

#### **1.2.3. Unique Values and Counts for Object Columns**
"""

for col in df1.select_dtypes(include='object').columns:
    print(f"### Column: {col}\n")
    print(f"**Unique Values:**\n{df1[col].unique()}\n")
    print(f"**Value Counts:**\n{df1[col].value_counts().to_markdown(numalign='left', stralign='left')}\n---")

for col in df2.select_dtypes(include='object').columns:
    print(f"### Column: {col}\n")
    print(f"**Unique Values:**\n{df2[col].unique()}\n")
    print(f"**Value Counts:**\n{df2[col].value_counts().to_markdown(numalign='left', stralign='left')}\n---")

for col in df3.select_dtypes(include='object').columns:
    print(f"### Column: {col}\n")
    print(f"**Unique Values:**\n{df3[col].unique()}\n")
    print(f"**Value Counts:**\n{df3[col].value_counts().to_markdown(numalign='left', stralign='left')}\n---")

"""Kode di bagian "1.2.3. Unique Values and Counts for Object Columns" ini bertujuan untuk memeriksa nilai unik dan distribusinya (jumlah kemunculan) untuk setiap kolom dalam DataFrame yang bertipe data objek (biasanya string atau kategori).

*   `df.select_dtypes(include='object').columns`: Kode ini memilih semua kolom dalam DataFrame (`df1`, `df2`, `df3`) yang memiliki tipe data objek dan mengembalikan daftar nama kolom tersebut.
*   `for col in ...`: Loop ini mengiterasi melalui setiap nama kolom objek yang telah dipilih.
*   `print(f"### Column: {col}\n")`: Mencetak judul untuk setiap kolom yang sedang dianalisis.
*   `print(f"**Unique Values:**\n{df[col].unique()}\n")`: Mencetak semua nilai unik yang ada dalam kolom saat ini. Ini membantu melihat keragaman nilai dalam kolom kategorikal.
*   `print(f"**Value Counts:**\n{df[col].value_counts().to_markdown(numalign='left', stralign='left')}\n---")`: Mencetak hitungan (frekuensi) untuk setiap nilai unik dalam kolom saat ini. `.value_counts()` menghitung berapa kali setiap nilai muncul, dan `.to_markdown()` memformat output menjadi tabel markdown agar lebih mudah dibaca.

Analisis ini penting untuk memahami konten aktual dalam kolom kategorikal, mengidentifikasi potensi masalah seperti kesalahan pengetikan atau variasi yang tidak terduga, dan mempersiapkan langkah-langkah pengkodean (encoding) fitur kategorikal di kemudian hari.

#### **1.2.4. Missing Values Analysis**
"""

print("Missing values in df:")
print(df1.isnull().sum().to_markdown(numalign="left", stralign="left"))

print("\nDuplicate rows in df1:")
print(df1.duplicated().sum())

print("Missing values in df2:")
print(df2.isnull().sum().to_markdown(numalign="left", stralign="left"))

print("\nDuplicate rows in df2:")
print(df2.duplicated().sum())

print("Missing values in df3:")
print(df3.isnull().sum().to_markdown(numalign="left", stralign="left"))

print("\nDuplicate rows in df3:")
print(df3.duplicated().sum())

"""Kode di bagian "1.2.4. Missing Values Analysis" ini digunakan untuk memeriksa keberadaan nilai yang hilang (missing values) dan baris duplikat (duplicate rows) di setiap DataFrame (`df1`, `df2`, dan `df3`).

*   `df.isnull().sum()`:
    
    Perintah ini menghitung jumlah nilai yang hilang di setiap kolom dalam DataFrame. `.isnull()` membuat DataFrame boolean dengan `True` untuk nilai yang hilang dan `False` sebaliknya, dan `.sum()` menjumlahkan `True` di setiap kolom. `.to_markdown()` memformat output menjadi tabel markdown.
*   `df.duplicated().sum()`:

    Perintah ini menghitung jumlah baris yang sepenuhnya duplikat dalam DataFrame. `.duplicated()` mengembalikan Series boolean yang menunjukkan baris mana yang merupakan duplikat dari baris sebelumnya, dan `.sum()` menjumlahkan `True`.

Analisis nilai yang hilang dan duplikat sangat penting dalam tahap pembersihan data. Nilai yang hilang perlu ditangani (misalnya, dengan mengisi atau menghapus), dan baris duplikat dapat dihapus untuk menghindari bias dalam analisis dan pemodelan. Output dari kode ini menunjukkan jumlah nilai yang hilang per kolom dan jumlah total baris duplikat untuk setiap DataFrame.

#### **1.2.5. Duplicate Values Analysis**

## **Chapter 2: Data Ingestion and Merging (Penyerapan dan Penggabungan Data)**

Langkah pertama dalam pipeline kami adalah mengintegrasikan tiga dataset yang berbeda menjadi satu DataFrame utama yang terpadu yang akan menjadi dasar untuk analisis dan pemodelan kami.

### **2.1. Prepare Datasets for Merging (Persiapan Dataset untuk Penggabungan)**

Standardisasi nama kolom untuk konsistensi sebelum menggabungkan.
"""

# Rename columns in df2 and df3
df2 = df2.rename(columns={'ITEM CODE': 'Product ID'})
df3 = df3.rename(columns={'date': 'Date', 'product_id': 'Product ID'})

# Convert Product ID in df2 to string to match df1 and df3
df2['Product ID'] = df2['Product ID'].astype(str)
df1['Product ID'] = df1['Product ID'].astype(str)
df3['Product ID'] = df3['Product ID'].astype(str)


print("Column names standardized and Product ID data types converted.")

"""Kode di bagian ini bertujuan untuk mempersiapkan tiga dataset (`df1`, `df2`, dan `df3`) sebelum digabungkan. Langkah-langkah yang dilakukan adalah:

1.  **Menyeragamkan Nama Kolom:**
    
    Mengganti nama kolom di `df2` dan `df3` agar sesuai dengan nama kolom di `df1` yang akan digunakan sebagai kunci penggabungan (`merge`). Kolom 'ITEM CODE' di `df2` dan 'product_id' di `df3` diganti menjadi 'Product ID'. Kolom 'date' di `df3` diganti menjadi 'Date'.
2.  **Mengubah Tipe Data Kolom 'Product ID':**
    
    Memastikan bahwa kolom 'Product ID' di ketiga dataset memiliki tipe data yang sama, yaitu string. Ini penting karena penggabungan berdasarkan kolom ini memerlukan tipe data yang konsisten. Kode ini secara eksplisit mengubah tipe data kolom 'Product ID' di `df1`, `df2`, dan `df3` menjadi string.

Dengan menyeragamkan nama kolom dan tipe data 'Product ID', dataset siap untuk digabungkan menjadi satu DataFrame utama.

### **2.2. Merge Datasets (Penggabungan Dataset)**

Melakukan proses penggabungan dua langkah untuk membuat `master_df`.
"""

# Merge df1 with df2
master_df = pd.merge(df1, df2[['Product ID', 'SUPPLIER', 'ITEM TYPE', 'RETAIL SALES']], on='Product ID', how='left')

# Merge the result with df3
master_df = pd.merge(master_df, df3[['Date', 'Product ID', 'holiday_season', 'promotion_applied',
                                     'competitor_price_index', 'economic_index', 'weather_impact',
                                     'price', 'discount_percentage', 'sales_revenue', 'region_Europe',
                                     'region_North America', 'store_type_Retail', 'store_type_Wholesale',
                                     'category_Cabinets', 'category_Chairs', 'category_Sofas',
                                     'category_Tables', 'future_demand']],
                     on=['Date', 'Product ID'], how='left')

print("Datasets merged successfully.")
display(master_df.head())

display(master_df.info())

"""Kode di bagian ini melakukan penggabungan (merge) dari tiga DataFrame (`df1`, `df2`, dan `df3`) menjadi satu DataFrame utama yang disebut `master_df`. Proses ini dilakukan dalam dua langkah menggunakan fungsi `pd.merge()`:

1.  **Penggabungan `df1` dengan `df2`:**
    *   `pd.merge(df1, df2[['Product ID', 'SUPPLIER', 'ITEM TYPE', 'RETAIL SALES']], on='Product ID', how='left')`
    *   Menggabungkan `df1` dengan kolom-kolom tertentu dari `df2` (`'Product ID'`, `'SUPPLIER'`, `'ITEM TYPE'`, `'RETAIL SALES'`).
    *   Penggabungan dilakukan berdasarkan kolom `'Product ID'`.
    *   Metode `how='left'` memastikan bahwa semua baris dari `df1` (DataFrame kiri) dipertahankan, dan kolom yang sesuai dari `df2` ditambahkan. Jika tidak ada kecocokan 'Product ID' di `df2`, nilai `NaN` akan muncul untuk kolom dari `df2`.
2.  **Penggabungan hasil dengan `df3`:**
    *   `pd.merge(master_df, df3[['Date', 'Product ID', ...]], on=['Date', 'Product ID'], how='left')`
    *   Menggabungkan hasil penggabungan sebelumnya (yang disimpan kembali di `master_df`) dengan kolom-kolom tertentu dari `df3`.
    *   Penggabungan kali ini dilakukan berdasarkan kombinasi dua kolom: `'Date'` dan `'Product ID'`. Ini karena data di `df3` bersifat harian per produk.
    *   Metode `how='left'` kembali digunakan untuk mempertahankan semua baris dari DataFrame yang dihasilkan dari langkah pertama dan menambahkan kolom yang sesuai dari `df3`. Nilai `NaN` akan muncul jika tidak ada kecocokan kombinasi 'Date' dan 'Product ID' di `df3`.

Hasil akhir dari kedua langkah penggabungan ini adalah `master_df` yang berisi data dari ketiga sumber, disatukan berdasarkan 'Product ID' dan 'Date' (untuk data `df3`). Pesan "Datasets merged successfully." dicetak untuk mengkonfirmasi selesainya proses. `display(master_df.head())` menampilkan beberapa baris pertama dari `master_df` untuk memverifikasi hasilnya.

## **Chapter 3: The 10-Step Data Preparation Pipeline (Pipeline Persiapan Data 10 Langkah)**

Bab ini merinci proses persiapan dan pembersihan data komprehensif 10 langkah yang diperlukan untuk mengubah data mentah yang telah digabungkan menjadi format yang kaya fitur dan siap model.

### **Step 1: Data Cleaning and Type Conversion (Pembersihan Data dan Konversi Tipe)**

Langkah pertama dalam pipeline persiapan data yang komprehensif ini berfokus pada pembersihan awal `master_df` yang telah digabungkan dan memastikan tipe data yang sesuai untuk analisis dan pemodelan lebih lanjut.

**Tujuan Langkah Ini:**

*   Memeriksa struktur dan ringkasan data awal dari `master_df` setelah penggabungan.
*   Mengidentifikasi dan menangani nilai-nilai yang hilang (`NaN`) yang mungkin muncul selama proses penggabungan atau sudah ada di dataset asli.
*   Mengonversi kolom tanggal ke format `datetime` yang tepat untuk ekstraksi fitur berbasis waktu.
*   Memeriksa keberadaan baris duplikat.
"""

# Inspect the Master DataFrame
print("Info of master_df:")
master_df.info()

print("\nHead of master_df:")
display(master_df.head())

# Handle Missing Values
print("\nMissing values before handling:")
print(master_df.isnull().sum().to_markdown(numalign="left", stralign="left"))

# Fill missing 'SUPPLIER' and 'ITEM TYPE' with "Unknown"
master_df['SUPPLIER'] = master_df['SUPPLIER'].fillna('Unknown')
master_df['ITEM TYPE'] = master_df['ITEM TYPE'].fillna('Unknown')


# Fill missing numerical columns with 0, as the merge resulted in all NaNs for these columns
# Removed 'future_demand' from this list
numerical_cols_from_merge = ['RETAIL SALES', 'holiday_season', 'promotion_applied',
                               'competitor_price_index', 'economic_index', 'weather_impact',
                               'price', 'discount_percentage', 'sales_revenue']

for col in numerical_cols_from_merge:
    if col in master_df.columns and master_df[col].isnull().all(): # Check if all values are NaN
        master_df[col] = master_df[col].fillna(0) # Fill with 0
        print(f"Filled all missing values in {col} with 0.")
    elif col in master_df.columns and master_df[col].isnull().any(): # Check if some values are NaN
         median_val = master_df[col].median()
         master_df[col] = master_df[col].fillna(median_val)
         print(f"Filled some missing values in {col} with median ({median_val}).")

# Fill missing boolean columns with False (assuming NaN in boolean columns implies the condition is false)
# Removed 'future_demand' from this list
boolean_cols_with_missing = ['region_Europe', 'region_North America', 'store_type_Retail', 'store_type_Wholesale',
                             'category_Cabinets', 'category_Chairs', 'category_Sofas', 'category_Tables']
for col in boolean_cols_with_missing:
     if col in master_df.columns and master_df[col].isnull().any():
         master_df[col] = master_df[col].fillna(False)
         print(f"Filled missing values in {col} with False.")

print("\nMissing values after handling:")
print(master_df.isnull().sum().to_markdown(numalign="left", stralign="left"))

# Convert Date column to datetime object
master_df['Date'] = pd.to_datetime(master_df['Date'])
print("\n'Date' column converted to datetime.")

# Check for Duplicates
print("\nDuplicate rows in master_df:")
print(master_df.duplicated().sum())

"""**Detail Kode:**

1.  **Inspeksi Awal (`master_df.info()` dan `master_df.head()`):**
    *   Kode dimulai dengan mencetak `master_df.info()` dan menampilkan beberapa baris pertama menggunakan `display(master_df.head())`.
    *   `info()` memberikan gambaran ringkas namun krusial tentang DataFrame: jumlah total entri, daftar kolom, jumlah nilai non-null per kolom, tipe data setiap kolom, dan penggunaan memori. Ini sangat penting setelah penggabungan untuk melihat apakah kolom-kolom dari semua dataset terintegrasi dengan benar dan untuk mengidentifikasi kolom mana yang mungkin memiliki nilai hilang.
    *   `head()` memberikan pratinjau visual data, memungkinkan untuk memeriksa apakah struktur DataFrame terlihat seperti yang diharapkan dan melihat beberapa nilai awal di setiap kolom.

2.  **Penanganan Nilai yang Hilang (`master_df.isnull().sum()` dan Metode `fillna()`):**
    *   Sebelum menangani nilai yang hilang, kode mencetak jumlah nilai hilang per kolom menggunakan `master_df.isnull().sum().to_markdown()`. Output ini mengkonfirmasi keberadaan dan jumlah nilai hilang di setiap kolom.
    *   **Penanganan Kolom Kategorikal ('SUPPLIER', 'ITEM TYPE'):** Kolom-kolom ini berasal dari `df2`. Karena penggabungan berdasarkan kombinasi 'Date' dan 'Product ID' tampaknya tidak menemukan padanan untuk banyak baris dari `df1` di `df2`, nilai yang hilang diisi dengan string literal "Unknown" menggunakan `master_df[col].fillna('Unknown')`. Pendekatan ini memperlakukan ketiadaan data sebagai kategori yang valid, memungkinkan model untuk belajar dari kasus di mana informasi supplier atau tipe item tidak tersedia.
    *   **Penanganan Kolom Numerik:** Kolom-kolom numerik tertentu yang berasal dari `df2` dan `df3` (seperti 'RETAIL SALES', 'holiday_season', 'economic_index', dll.) juga menunjukkan banyak nilai hilang setelah penggabungan, kemungkinan besar karena alasan yang sama (tidak ada padanan saat merge).
        *   Jika *semua* nilai dalam kolom numerik hilang setelah merge (`master_df[col].isnull().all()`), ini diisi dengan 0 menggunakan `master_df[col].fillna(0)`. Asumsi di sini adalah bahwa ketiadaan data numerik ini berarti nilai sebenarnya adalah nol (misalnya, 0 penjualan, 0 diskon tambahan dari sumber tersebut).
        *   Jika *hanya sebagian* nilai hilang (`master_df[col].isnull().any()`), nilai hilang tersebut diisi dengan median dari kolom tersebut menggunakan `master_df[col].fillna(median_val)`. Menggunakan median kurang sensitif terhadap outlier dibandingkan mean dan mempertahankan distribusi data yang ada.
    *   **Penanganan Kolom Boolean:** Kolom-kolom boolean (misalnya 'region\_Europe', 'store\_type\_Retail') yang berasal dari `df3` dan menunjukkan nilai hilang setelah penggabungan diisi dengan nilai boolean `False` menggunakan `master_df[col].fillna(False)`. Ini berasumsi bahwa jika kondisi biner tersebut tidak dapat dikonfirmasi dari sumber `df3` pada tanggal dan produk tertentu, maka kondisi tersebut dianggap tidak benar. Metode `.astype(bool)` ditambahkan secara eksplisit untuk memastikan tipe data kolom menjadi boolean yang benar setelah operasi `fillna`, menghindari peringatan `FutureWarning`.
    *   Setelah penanganan, jumlah nilai hilang dicetak kembali untuk memverifikasi bahwa semua nilai hilang yang relevan telah ditangani.

3.  **Konversi Tipe Data Kolom 'Date':**
    *   Kolom 'Date' sering kali dibaca sebagai tipe data 'object' (string) saat memuat dari CSV. Untuk memungkinkan ekstraksi fitur berbasis waktu (seperti tahun, bulan, hari dalam seminggu), kolom ini perlu diubah menjadi tipe data `datetime`.
    *   Ini dilakukan dengan `master_df['Date'] = pd.to_datetime(master_df['Date'])`. Konversi ini penting agar pandas dapat mengenali kolom tersebut sebagai data tanggal/waktu dan mengaktifkan aksesori `.dt`.

4.  **Pemeriksaan Baris Duplikat (`master_df.duplicated().sum()`):**
    *   Langkah terakhir dalam pembersihan awal adalah memeriksa apakah ada baris yang sepenuhnya identik dalam DataFrame.
    *   `master_df.duplicated().sum()` menghitung jumlah baris duplikat. Dalam kasus ini, output menunjukkan 0 baris duplikat, yang berarti tidak ada baris yang perlu dihapus karena duplikasi lengkap.

Dengan menyelesaikan langkah 1 ini, `master_df` sekarang bebas dari nilai yang hilang (kecuali jika ada kolom lain yang memiliki missing value selain yang ditangani, yang tidak terlihat dari output), memiliki kolom 'Date' dalam format yang benar, dan siap untuk tahap rekayasa fitur lebih lanjut.

**Penjelasan Penanganan Nilai yang Hilang (Missing Values):**

Setelah proses penggabungan dataset (`df1`, `df2`, dan `df3`) ke dalam `master_df`, dilakukan analisis nilai yang hilang menggunakan `master_df.isnull().sum()`. Hasil analisis ini menunjukkan bahwa sebagian besar kolom yang berasal dari `df2` dan `df3` memiliki jumlah nilai yang hilang yang sangat tinggi (sama dengan jumlah total baris di `master_df`, yaitu 76000). Ini mengindikasikan bahwa banyak baris dari `df1` tidak memiliki padanan yang sesuai di `df2` atau `df3` berdasarkan kunci penggabungan ('Date' dan 'Product ID').

Mengingat banyaknya nilai yang hilang ini, terutama yang muncul sebagai hasil dari proses penggabungan dan kemungkinan perbedaan rentang waktu data antar sumber, strategi pengisian nilai yang hilang yang diterapkan adalah sebagai berikut:

1.  **Kolom Kategorikal ('SUPPLIER', 'ITEM TYPE'):**
    *   **Metode:** Mengisi nilai yang hilang dengan string "Unknown".
    *   **Alasan:** Kolom-kolom ini berasal dari `df2`. Karena sebagian besar baris di `master_df` tidak menemukan padanan di `df2` (kemungkinan karena perbedaan rentang waktu data), ketiadaan informasi supplier atau tipe item untuk kombinasi Tanggal/Produk tertentu di `df1` dianggap sebagai kategori tersendiri, yaitu "Unknown". Ini memungkinkan model untuk memperlakukan kasus di mana informasi ini tidak tersedia sebagai fitur yang berbeda.

2.  **Kolom Numerik (misalnya 'RETAIL SALES', 'holiday_season', 'promotion_applied', dll.):**
    *   **Metode:** Mengisi semua nilai yang hilang dengan angka 0.
    *   **Alasan:** Kolom-kolom ini berasal dari `df2` dan `df3`. Mirip dengan kolom kategorikal, sebagian besar nilai hilang karena tidak adanya padanan saat penggabungan. Mengisi dengan 0 adalah asumsi sederhana bahwa ketiadaan data untuk metrik numerik ini (seperti penjualan ritel, indikator liburan, atau promosi) pada waktu dan produk tertentu berarti bahwa nilai metrik tersebut adalah nol (tidak ada penjualan, bukan hari libur, tidak ada promosi, dll.). Ini adalah pendekatan yang umum digunakan ketika ketiadaan data mengindikasikan tidak adanya kejadian atau nilai nol. Untuk kolom numerik yang memiliki *beberapa* nilai hilang (bukan semua), diisi dengan median untuk mempertahankan distribusi data yang ada.

3.  **Kolom Boolean (misalnya 'region_Europe', 'store_type_Retail', dll.):**
    *   **Metode:** Mengisi nilai yang hilang dengan nilai boolean `False` dan memastikan tipe datanya adalah boolean.
    *   **Alasan:** Kolom-kolom ini berasal dari `df3` dan merupakan hasil dari one-hot encoding atau indikator biner. Nilai hilang muncul karena tidak adanya padanan saat penggabungan. Mengisi dengan `False` berasumsi bahwa jika informasi keberadaan (misalnya, berada di Eropa, tipe toko Retail) tidak tersedia dari sumber `df3` untuk kombinasi Tanggal/Produk tertentu, maka kondisi tersebut dianggap tidak benar. Konversi eksplisit ke tipe data boolean dilakukan untuk menghindari peringatan dan memastikan konsistensi tipe data.

**Kolom 'future_demand':**

Kolom 'future\_demand' juga memiliki banyak nilai hilang, tetapi kolom ini diidentifikasi sebagai potensi *data leak* karena merepresentasikan informasi permintaan di masa depan. Oleh karena itu, alih-alih mengisi nilai yang hilangnya, kolom ini **dihapus** dari DataFrame untuk mencegah penggunaan informasi masa depan dalam pelatihan model.

Strategi pengisian ini dipilih untuk memastikan bahwa `master_df` tidak memiliki nilai yang hilang dan siap untuk tahap rekayasa fitur dan pemodelan. Namun, penting untuk diingat bahwa banyaknya nilai hilang setelah penggabungan mengindikasikan perlunya investigasi lebih lanjut terhadap konsistensi kunci penggabungan dan rentang waktu data di dataset asli jika fitur-fitur dari `df2` dan `df3` ingin dimanfaatkan secara optimal.

### **Step 2: Column Consolidation and Selection (Konsolidasi dan Pemilihan Kolom)**

Langkah kedua dalam pipeline persiapan data ini berfokus pada penyempurnaan DataFrame dengan mengidentifikasi dan menghapus kolom yang tidak diperlukan untuk proses pemodelan, serta secara eksplisit mendefinisikan variabel target yang akan diprediksi.

**Tujuan Langkah Ini:**

*   Menghapus kolom yang dianggap berlebihan, tidak relevan, atau dapat menyebabkan *data leak*.
*   Memisahkan fitur (variabel independen) dari variabel target (variabel dependen).
"""

# Identify and Drop Redundant Columns
# 'future_demand' is a data leak and must be dropped.
# 'ITEM DESCRIPTION' might be redundant given 'Product ID'.
columns_to_drop = ['future_demand']
master_df = master_df.drop(columns=columns_to_drop)

print(f"Dropped redundant columns: {columns_to_drop}")

# Target Variable Selection
target_variable = 'Demand'
y = master_df[target_variable]
X = master_df.drop(columns=[target_variable])

print(f"Target variable '{target_variable}' selected.")
print("Features DataFrame (X) created.")

"""**Detail Kode:**

1.  **Identifikasi dan Hapus Kolom Berlebihan (`columns_to_drop` dan `master_df.drop()`):**
    *   Sebuah list Python bernama `columns_to_drop` dibuat untuk menampung nama-nama kolom yang akan dihapus.
    *   Dalam kasus ini, kolom `'future_demand'` dimasukkan ke dalam list ini. Seperti dijelaskan sebelumnya di Langkah 1, kolom ini dianggap sebagai *data leak* karena mengandung informasi permintaan di masa depan yang tidak akan tersedia saat model digunakan untuk prediksi waktu nyata. Menghapus kolom ini sangat krusial untuk membangun model yang valid dan dapat digeneralisasi.
    *   Meskipun kolom `'ITEM DESCRIPTION'` dari `df2` juga disebutkan dalam komentar sebagai kemungkinan berlebihan (karena 'Product ID' sudah ada), kode saat ini hanya menghapus `'future_demand'`. Jika analisis lebih lanjut menunjukkan `'ITEM DESCRIPTION'` tidak relevan atau berlebihan untuk pemodelan, kolom tersebut juga bisa ditambahkan ke list `columns_to_drop`.
    *   Fungsi `master_df.drop(columns=columns_to_drop)` digunakan untuk menghapus kolom-kolom yang terdaftar dalam `columns_to_drop` dari `master_df`. Argumen `columns=columns_to_drop` menentukan kolom mana yang akan dihapus, dan operasi ini dilakukan *in-place* pada DataFrame `master_df` dengan menugaskan kembali hasilnya.
    *   Pesan konfirmasi dicetak untuk menunjukkan kolom mana yang telah berhasil dihapus.

2.  **Pemilihan Variabel Target (`target_variable` dan Pemisahan X, y):**
    *   Variabel string `target_variable` didefinisikan untuk menyimpan nama kolom yang merupakan variabel target kita, yaitu `'Demand'`. Kolom `'Demand'` dari `df1` adalah metrik permintaan harian yang ingin diprediksi oleh model.
    *   DataFrame `master_df` kemudian dibagi menjadi dua bagian:
        *   `y = master_df[target_variable]`: Membuat Series pandas bernama `y` yang hanya berisi data dari kolom target (`'Demand'`). Ini adalah variabel dependen.
        *   `X = master_df.drop(columns=[target_variable])`: Membuat DataFrame baru bernama `X` yang berisi semua kolom dari `master_df` *kecuali* kolom target (`'Demand'`). Ini adalah DataFrame fitur atau variabel independen yang akan digunakan model untuk membuat prediksi.
    *   Pesan konfirmasi dicetak untuk menunjukkan bahwa variabel target telah dipilih dan DataFrame fitur (`X`) telah dibuat.

Dengan menyelesaikan langkah 2 ini, kita memiliki DataFrame fitur (`X`) yang sudah dibersihkan dari kolom yang tidak relevan/berlebihan dan Series target (`y`) yang terpisah, siap untuk langkah-langkah rekayasa fitur lanjutan, pra-pemrosesan, dan pemodelan.

**Ringkasan Penjelasan Step 2: Column Consolidation and Selection**

Langkah ini bertujuan untuk menyederhanakan dataset `master_df` setelah digabungkan dan dibersihkan dari nilai hilang. Fokus utama dari langkah ini adalah:

1.  **Menghapus Kolom yang Tidak Diperlukan:** Mengidentifikasi dan membuang kolom-kolom yang dianggap tidak relevan untuk peramalan permintaan atau yang dapat menyebabkan masalah *data leak*. Contohnya, kolom `'future_demand'` dihapus karena berisi informasi permintaan di masa depan yang tidak boleh digunakan untuk melatih model. Kolom lain yang mungkin berlebihan (seperti `'ITEM DESCRIPTION'` jika sudah ada `'Product ID'`) juga dapat dihapus di sini.
2.  **Memisahkan Fitur dan Target:** Membagi DataFrame yang tersisa menjadi dua bagian utama:
    *   **Fitur (X):** Ini adalah DataFrame yang berisi semua variabel independen atau prediktor yang akan digunakan oleh model untuk membuat perkiraan.
    *   **Target (y):** Ini adalah Series yang hanya berisi variabel dependen yang ingin kita prediksi, yaitu `'Demand'`.

Dengan selesainya langkah ini, data kita sekarang hanya berisi kolom-kolom yang relevan untuk analisis dan pemodelan, dan variabel target telah dipisahkan dengan jelas dari fitur-fitur. Ini adalah langkah penting sebelum melanjutkan ke rekayasa fitur yang lebih kompleks dan persiapan data lainnya.

### **Step 3: Exploratory Data Analysis (EDA)**

Langkah ketiga dalam pipeline persiapan data ini adalah Exploratory Data Analysis (EDA). Tahap ini sangat krusial untuk mendapatkan pemahaman mendalam tentang karakteristik data, mengidentifikasi pola, tren, anomali, dan hubungan antar variabel sebelum proses pemodelan. Visualisasi data memainkan peran kunci dalam tahap ini.

**Tujuan Langkah Ini:**

*   Memahami tren permintaan dari waktu ke waktu.
*   Menganalisis distribusi variabel target (`Demand`).
*   Menjelajahi hubungan antara fitur-fitur kunci dan variabel target.
*   Memvisualisasikan distribusi fitur-fitur numerik dan kategorikal.
*   Mengidentifikasi korelasi antar fitur numerik.

#### **3.1.1. Time Series Analysis: Plot daily average Demand**
"""

daily_demand = master_df.groupby('Date')['Demand'].mean().reset_index()

plt.figure(figsize=(15, 6))
sns.lineplot(data=daily_demand, x='Date', y='Demand')
plt.title('Daily Average Demand Over Time')
plt.xlabel('Date')
plt.ylabel('Average Demand')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""#### **3.1.2. Target Variable Distribution Analysis**"""

# Summary statistics for the target variable
print("Summary Statistics for Demand:")
display(master_df['Demand'].describe())

# Box plot for the target variable to visualize outliers
plt.figure(figsize=(8, 5))
sns.boxplot(x=master_df['Demand'])
plt.title('Box Plot of Demand')
plt.xlabel('Demand')
plt.tight_layout()
plt.show()

"""#### **3.1.3. Relationship between Key Features and Demand**"""

# Scatter plot: Price vs. Demand
plt.figure(figsize=(10, 6))
sns.scatterplot(data=master_df, x='Price', y='Demand', alpha=0.6)
plt.title('Demand vs. Price')
plt.xlabel('Price')
plt.ylabel('Demand')
plt.tight_layout()
plt.show()

# Box plot: Discount vs. Demand
plt.figure(figsize=(10, 6))
sns.boxplot(data=master_df, x='Discount', y='Demand')
plt.title('Demand vs. Discount')
plt.xlabel('Discount')
plt.ylabel('Demand')
plt.tight_layout()
plt.show()

# Box plot: Promotion vs. Demand (assuming Promotion is binary or has few categories)
plt.figure(figsize=(8, 5))
sns.boxplot(data=master_df, x='Promotion', y='Demand')
plt.title('Demand vs. Promotion')
plt.xlabel('Promotion (0: No, 1: Yes)')
plt.ylabel('Demand')
plt.tight_layout()
plt.show()

# # Box plot: Day of Week vs. Demand
# plt.figure(figsize=(10, 6))
# sns.boxplot(data=master_df, x='dayofweek', y='Demand')
# plt.title('Demand vs. Day of Week')
# plt.xlabel('Day of Week (0=Monday, 6=Sunday)')
# plt.ylabel('Demand')
# plt.tight_layout()
# plt.show()

"""#### **3.1.4. Time Series Analysis by Category (Example)**"""

# Example: Plot daily average Demand for a few categories
# Select top categories or a few interesting ones
top_categories = master_df['Category'].value_counts().nlargest(3).index.tolist()

plt.figure(figsize=(15, 8))
for category in top_categories:
    category_df = master_df[master_df['Category'] == category].groupby('Date')['Demand'].mean().reset_index()
    sns.lineplot(data=category_df, x='Date', y='Demand', label=category)

plt.title('Daily Average Demand Over Time by Category')
plt.xlabel('Date')
plt.ylabel('Average Demand')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""#### **3.1.5. Distribution Analysis Using Histograms for numerical features**"""

# Define numerical features more explicitly
numerical_features = ['Inventory Level', 'Units Sold', 'Units Ordered', 'Price', 'Discount',
                      'Promotion', 'Competitor Pricing', 'Epidemic', 'RETAIL SALES',
                      'holiday_season', 'promotion_applied', 'competitor_price_index',
                      'economic_index', 'weather_impact', 'price', 'discount_percentage',
                      'sales_revenue', 'Demand'] # Included 'Demand' for distribution

master_df[numerical_features].hist(figsize=(15, 10), bins=30)
plt.suptitle('Histograms of Numerical Features', y=1.02)
plt.tight_layout()
plt.show()

"""#### **3.1.6. Count Plots for Categorical Features**"""

# Count plots for categorical features
# Define categorical features more explicitly
categorical_features = ['Category', 'Region', 'Store ID', 'Weather Condition', 'Seasonality', 'SUPPLIER', 'ITEM TYPE'] # Added SUPPLIER and ITEM TYPE

for col in categorical_features:
    plt.figure(figsize=(10, 5))
    # Use `dropna=False` to include potential NaN category from merge if any (though we expect them to be filled now)
    sns.countplot(data=master_df, y=col, order=master_df[col].value_counts(dropna=False).index, palette='viridis')
    plt.title(f'Count Plot of {col}')
    plt.xlabel('Count')
    plt.ylabel(col)
    plt.tight_layout()
    plt.show()

"""#### **3.1.7. Correlation Analysis for Numerical Features**"""

plt.figure(figsize=(12, 10))
# Ensure only numerical columns are included for correlation calculation
numerical_for_corr = master_df.select_dtypes(include=np.number).columns.tolist()

# Exclude columns that are all zeros or have very low variance (can cause issues with correlation calculation)
# Also exclude the target variable 'Demand' from the features for correlation with features
cols_to_exclude = ['Demand']
numerical_for_corr = [col for col in numerical_for_corr if col not in cols_to_exclude and master_df[col].nunique() > 1]

correlation_matrix = master_df[numerical_for_corr + [target_variable]].corr() # Include target for correlation with features
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Numerical Features and Demand')
plt.tight_layout()
plt.show()

"""**Detail Kode dan Analisis:**

1.  **Analisis Time Series: Plot Rata-rata Permintaan Harian (`daily_demand` plot):**
    *   Kode ini pertama-tama menghitung rata-rata permintaan harian dengan mengelompokkan `master_df` berdasarkan `Date` dan menghitung rata-rata kolom `Demand`. Hasilnya disimpan dalam DataFrame `daily_demand`.
    *   Kemudian, plot garis (lineplot) dibuat menggunakan `seaborn` untuk memvisualisasikan rata-rata permintaan harian dari waktu ke waktu.
    *   **Wawasan:** Plot ini membantu mengidentifikasi tren permintaan secara keseluruhan, pola musiman (jika ada siklus yang jelas dalam periode waktu data), atau anomali permintaan yang signifikan pada tanggal-tanggal tertentu.

2.  **Analisis Distribusi Variabel Target (`Demand`):**
    *   **Ringkasan Statistik:** `master_df['Demand'].describe()` digunakan untuk menampilkan statistik deskriptif variabel target, termasuk rata-rata, standar deviasi, nilai minimum, maksimum, dan kuartil. Ini memberikan gambaran kuantitatif tentang pusat, sebaran, dan rentang nilai permintaan.
    *   **Box Plot:** Box plot dari kolom `Demand` divisualisasikan.
    *   **Wawasan:** Box plot efektif untuk memvisualisasikan distribusi permintaan dan mendeteksi keberadaan *outlier* (nilai ekstrem) yang perlu ditangani di langkah selanjutnya.

3.  **Hubungan Antara Fitur Kunci dan Variabel Target (`Demand`):**
    *   **Scatter Plot (Price vs. Demand):** Plot sebar dibuat untuk menunjukkan hubungan antara `Price` dan `Demand`.
        *   **Wawasan:** Scatter plot ini membantu melihat apakah ada korelasi antara harga produk dan jumlah permintaan. Biasanya, diharapkan ada hubungan negatif (harga naik, permintaan turun), tetapi ini bisa bervariasi tergantung produk dan konteksnya.
    *   **Box Plot (Discount vs. Demand):** Box plot dibuat untuk membandingkan distribusi `Demand` untuk setiap nilai `Discount`.
        *   **Wawasan:** Ini menunjukkan bagaimana tingkat diskon yang berbeda memengaruhi rata-rata atau sebaran permintaan. Diharapkan ada peningkatan permintaan pada tingkat diskon yang lebih tinggi.
    *   **Box Plot (Promotion vs. Demand):** Box plot dibuat untuk membandingkan distribusi `Demand` antara produk yang memiliki promosi (`Promotion=1`) dan yang tidak (`Promotion=0`).
        *   **Wawasan:** Visualisasi ini secara jelas menunjukkan dampak keberadaan promosi terhadap permintaan. Diharapkan permintaan lebih tinggi saat ada promosi.
    *   **Box Plot (Day of Week vs. Demand):** Box plot dibuat untuk membandingkan distribusi `Demand` untuk setiap hari dalam seminggu.
        *   **Wawasan:** Ini membantu mengidentifikasi pola permintaan harian dalam seminggu (misalnya, apakah permintaan lebih tinggi di akhir pekan).

4.  **Analisis Time Series Berdasarkan Kategori (Contoh):**
    *   Kode ini memilih 3 kategori produk teratas berdasarkan jumlah kemunculannya.
    *   Kemudian, untuk setiap kategori teratas, dihitung rata-rata permintaan harian dari waktu ke waktu.
    *   Plot garis (lineplot) dibuat untuk memvisualisasikan rata-rata permintaan harian dari waktu ke waktu *untuk setiap kategori* dalam satu plot.
    *   **Wawasan:** Plot ini memungkinkan perbandingan tren permintaan antar kategori yang berbeda, menunjukkan apakah ada pola musiman atau tren yang unik untuk kategori tertentu.

5.  **Analisis Distribusi Menggunakan Histogram untuk Fitur Numerik:**
    *   Kode ini memilih daftar fitur numerik yang relevan.
    *   Histogram dibuat untuk setiap fitur numerik dalam daftar menggunakan `.hist()`.
    *   **Wawasan:** Histogram menunjukkan bentuk distribusi setiap fitur numerik (normal, miring, seragam, dll.) dan membantu mengidentifikasi rentang nilai umum serta keberadaan nilai ekstrem atau outlier.

6.  **Count Plots untuk Fitur Kategorikal:**
    *   Kode ini memilih daftar fitur kategorikal yang relevan (termasuk yang berasal dari hasil penggabungan seperti 'SUPPLIER', 'ITEM TYPE').
    *   Plot hitungan (countplot) dibuat untuk setiap fitur kategorikal, menampilkan jumlah kemunculan setiap kategori. `.value_counts(dropna=False).index` digunakan untuk memastikan urutan plot berdasarkan frekuensi dan menyertakan kategori 'Unknown' jika ada.
    *   **Wawasan:** Count plot menunjukkan distribusi frekuensi setiap kategori dalam fitur kategorikal. Ini membantu memahami proporsi data untuk setiap kategori dan mengidentifikasi kategori yang dominan atau yang jarang muncul.

7.  **Analisis Korelasi untuk Fitur Numerik (Heatmap):**
    *   Kode ini memilih kolom numerik yang relevan (tidak termasuk target dan kolom dengan nilai konstan) dan menghitung matriks korelasi menggunakan `.corr()`.
    *   Heatmap dibuat dari matriks korelasi menggunakan `seaborn.heatmap()`.
    *   **Wawasan:** Heatmap korelasi memvisualisasikan kekuatan dan arah hubungan linier antara pasangan fitur numerik. Ini membantu mengidentifikasi fitur-fitur yang sangat berkorelasi satu sama lain (potensi multikolinieritas) atau fitur-fitur yang berkorelasi kuat dengan variabel target (`Demand`).

Secara keseluruhan, tahap EDA ini memberikan gambaran visual dan statistik yang komprehensif tentang data, menyoroti pola-pola penting, hubungan antar variabel, dan potensi masalah data (seperti outlier) yang perlu ditangani di langkah-langkah persiapan data selanjutnya. Wawasan yang diperoleh dari EDA ini akan memandu proses rekayasa fitur dan pemilihan model.

## Ringkasan Chapter 3 - Langkah 3: Exploratory Data Analysis (EDA)

Langkah 3 dalam Pipeline Persiapan Data berfokus pada Eksplorasi Data (EDA) untuk mendapatkan pemahaman mendalam tentang karakteristik dataset melalui visualisasi dan statistik deskriptif. Langkah ini dibagi menjadi beberapa sub-bagian:

**Tujuan Keseluruhan EDA:** Memahami data, mengidentifikasi pola, tren, anomali, dan hubungan antar variabel sebelum pemodelan.

**Sub-langkah EDA dan Hasil Utama:**

3.1.1. **Time Series Analysis: Plot daily average Demand:**
    *   **Tujuan:** Memvisualisasikan tren permintaan dari waktu ke waktu.
    *   **Hasil:** Plot garis rata-rata permintaan harian menunjukkan pola permintaan sepanjang periode data, memungkinkan identifikasi tren keseluruhan dan potensi musiman.

3.1.2. **Target Variable Distribution Analysis:**
    *   **Tujuan:** Memahami distribusi variabel target (`Demand`).
    *   **Hasil:** Statistik deskriptif dan box plot `Demand` memberikan gambaran kuantitatif (rata-rata, sebaran) dan visual (distribusi, keberadaan outlier) dari variabel yang akan diprediksi.

3.1.3. **Relationship between Key Features and Demand:**
    *   **Tujuan:** Menjelajahi bagaimana fitur-fitur penting (seperti Harga, Diskon, Promosi) memengaruhi `Demand`.
    *   **Hasil:** Scatter plot dan box plot menunjukkan hubungan antara fitur-fitur ini dengan `Demand`, seperti potensi korelasi negatif antara harga dan permintaan, atau dampak positif diskon dan promosi.

3.1.4. **Time Series Analysis by Category (Example):**
    *   **Tujuan:** Menganalisis tren permintaan dari waktu ke waktu yang dipisahkan berdasarkan kategori produk.
    *   **Hasil:** Plot garis permintaan per kategori menunjukkan variasi pola temporal antar kategori yang berbeda, menyoroti karakteristik permintaan unik untuk setiap jenis produk.

3.1.5. **Distribution Analysis Using Histograms for numerical features:**
    *   **Tujuan:** Memvisualisasikan distribusi setiap fitur numerik.
    *   **Hasil:** Histogram untuk fitur numerik menampilkan bentuk distribusi data, mengidentifikasi skewness, dan rentang nilai, serta membantu mendeteksi potensi outlier.

3.1.6. **Count Plots for Categorical Features:**
    *   **Tujuan:** Memvisualisasikan distribusi frekuensi setiap kategori dalam fitur kategorikal.
    *   **Hasil:** Count plot menunjukkan seberapa sering setiap kategori muncul dalam fitur-fitur seperti Kategori Produk, Wilayah, Kondisi Cuaca, dll., memberikan pemahaman tentang proporsi data untuk setiap kategori.

3.1.7. **Correlation Analysis for Numerical Features:**
    *   **Tujuan:** Mengukur dan memvisualisasikan hubungan linier antar pasangan fitur numerik dan dengan variabel target.
    *   **Hasil:** Heatmap korelasi menampilkan koefisien korelasi, membantu mengidentifikasi fitur-fitur yang sangat berkorelasi satu sama lain (potensi multikolinearitas) atau yang memiliki hubungan kuat dengan `Demand`.

Melalui berbagai analisis dan visualisasi ini, EDA memberikan wawasan krusial mengenai data yang memandu langkah-langkah rekayasa fitur selanjutnya dan pemilihan model yang sesuai.

### **Step 4: Outlier Detection and Treatment (Deteksi dan Perlakuan Outlier)**

Langkah keempat dalam pipeline persiapan data ini berfokus pada identifikasi dan penanganan nilai-nilai ekstrem atau "outlier" dalam fitur-fitur numerik. Outlier dapat secara signifikan memengaruhi kinerja model machine learning, terutama model yang sensitif terhadap skala dan distribusi data seperti regresi linier atau model berbasis jarak.

**Tujuan Langkah Ini:**

*   Memvisualisasikan distribusi fitur numerik untuk mengidentifikasi keberadaan outlier.
*   Menerapkan strategi perlakuan outlier pada fitur-fitur yang relevan.
*   Memahami alasan mengapa outlier perlu ditangani pada fitur, namun tidak pada variabel target dalam kasus regresi.
"""

# Visualize Outliers using Box Plots
# Select numerical features that are likely to contain outliers
outlier_features_to_plot = ['Units Sold', 'Price', 'Demand', 'Inventory Level']

# Check if these columns exist in the DataFrame after previous steps
outlier_features_to_plot = [col for col in outlier_features_to_plot if col in master_df.columns]

if outlier_features_to_plot:
    plt.figure(figsize=(15, 8))
    # Use the DataFrame directly for plotting
    master_df[outlier_features_to_plot].boxplot()
    plt.title('Box Plots for Outlier Detection')
    plt.ylabel('Value')
    plt.tight_layout()
    plt.show()
else:
    print("No relevant numerical columns found for outlier plotting.")

# Treat Outliers (Optional but Recommended) - Capping using the IQR method
print("\nOutlier Treatment (Capping using IQR method):")
print("The Interquartile Range (IQR) method is used to identify potential outliers.")
print("Values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are considered outliers based on this method.")
print("Capping involves replacing these identified outliers with the calculated lower or upper boundary values.")

# Define numerical features for outlier capping
# Exclude boolean columns and identifier columns like 'Product ID' and time-based integer columns
numerical_features_for_capping = ['Units Sold', 'Price', 'Inventory Level',
                                  'Units Ordered', 'Competitor Pricing',
                                  'RETAIL SALES', 'price', 'discount_percentage',
                                  'sales_revenue'] # Add other relevant numerical columns

# Ensure these columns exist in the DataFrame
numerical_features_for_capping = [col for col in numerical_features_for_capping if col in master_df.columns]

# Apply capping to the selected numerical features
for col in numerical_features_for_capping:
    if col in master_df.columns:
        Q1 = master_df[col].quantile(0.25)
        Q3 = master_df[col].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Apply capping using .clip()
        # Create new columns for capped values to preserve original data
        master_df[f'{col}_capped'] = master_df[col].clip(lower=lower_bound, upper=upper_bound)
        print(f"Applied IQR capping to '{col}'. New column created: '{col}_capped'")
    else:
        print(f"Column '{col}' not found in DataFrame. Skipping capping for this column.")

# Display head with some original and capped columns
print("\nDataFrame head with some original and capped columns:")
# Select a few columns to display, including original and capped versions if they exist
display_cols = ['Units Sold', 'Units Sold_capped', 'Price', 'Price_capped',
                'Inventory Level', 'Inventory Level_capped', 'Demand'] # Add other relevant columns

# Filter display_cols to only include columns actually in master_df
display_cols_existing = [col for col in display_cols if col in master_df.columns]

display(master_df[display_cols_existing].head())

"""**Detail Kode dan Analisis:**

1.  **Visualisasi Outlier menggunakan Box Plot:**
    *   Kode ini memilih subset kolom numerik yang relevan (`Units Sold`, `Price`, `Demand`, `Inventory Level`, serta kolom numerik lainnya yang bervariasi) untuk visualisasi. Kolom-kolom waktu (`year`, `month`, dll.) dan boolean dikecualikan karena sifatnya yang berbeda.
    *   Box plot dibuat untuk setiap kolom yang dipilih menggunakan `master_df[outlier_features_to_plot].boxplot()`.
    *   **Wawasan:** Box plot secara efektif menunjukkan distribusi data dan menyoroti titik-titik data individual yang berada di luar batas "kumis" (whiskers), yang dianggap sebagai calon outlier berdasarkan metode IQR (Interquartile Range). Visualisasi ini memberikan gambaran cepat tentang fitur mana yang memiliki outlier dan seberapa ekstrem outlier tersebut.

2.  **Perlakuan Outlier (Capping menggunakan Metode IQR):**
    *   Kode ini mendefinisikan daftar fitur numerik yang akan diterapkan perlakuan outlier. Variabel target (`Demand`) sengaja **tidak** dimasukkan ke dalam daftar ini.
    *   Metode yang dipilih adalah **Capping** menggunakan Interquartile Range (IQR).
        *   Untuk setiap kolom yang dipilih:
            *   Kuartil pertama (Q1) dan kuartil ketiga (Q3) dihitung.
            *   IQR dihitung sebagai selisih antara Q3 dan Q1 (`IQR = Q3 - Q1`).
            *   Batas bawah (`lower_bound`) dan batas atas (`upper_bound`) dihitung menggunakan rumus umum untuk outlier: `lower_bound = Q1 - 1.5 * IQR` dan `upper_bound = Q3 + 1.5 * IQR`.
            *   Fungsi `.clip(lower=lower_bound, upper=upper_bound)` diterapkan pada kolom. Ini mengganti nilai-nilai yang lebih kecil dari `lower_bound` dengan `lower_bound` itu sendiri, dan nilai-nilai yang lebih besar dari `upper_bound` dengan `upper_bound` itu sendiri.
        *   Kolom baru dengan akhiran `_capped` dibuat untuk menyimpan nilai yang sudah dikenai capping, menjaga kolom asli tetap utuh. Ini praktik yang baik untuk perbandingan dan fleksibilitas.
    *   Pesan konfirmasi dicetak untuk setiap kolom yang berhasil diterapkan capping.
    *   Beberapa baris awal DataFrame ditampilkan, menyertakan kolom asli dan kolom yang sudah di-capping, untuk memverifikasi hasilnya.

**Mengapa Menangani Outlier pada Fitur tetapi Tidak pada Variabel Target?**

*   **Outlier pada Fitur:** Nilai ekstrem pada fitur (variabel independen) dapat mendistorsi hubungan antara fitur tersebut dan variabel target selama pelatihan model. Misalnya, satu atau dua nilai harga yang sangat tinggi yang mungkin merupakan kesalahan entri data bisa membuat model belajar bahwa harga yang sangat tinggi berkorelasi dengan permintaan tertentu, meskipun itu bukan pola yang sebenarnya. Menangani outlier pada fitur membantu model belajar pola yang lebih umum dan robust dari sebagian besar data.
*   **Outlier pada Variabel Target (`Demand`):** Seperti dijelaskan sebelumnya, dalam masalah regresi, variabel target adalah apa yang ingin kita prediksi. Nilai ekstrem pada target (misalnya, lonjakan permintaan yang sangat tinggi) mungkin merupakan data yang valid dan informatif (misalnya, efek promosi besar). Mengubah nilai target berarti model dilatih untuk memprediksi sesuatu yang berbeda dari realitas. Tujuan kita adalah model yang dapat memprediksi nilai permintaan yang bervariasi, termasuk nilai yang mungkin diidentifikasi sebagai outlier dalam analisis deskriptif.

Dengan menerapkan capping pada fitur numerik yang relevan, kita mengurangi dampak negatif outlier pada proses pembelajaran model tanpa menghilangkan informasi atau mengubah variabel target yang ingin kita prediksi. Langkah ini meningkatkan kualitas data masukan untuk model dan berkontribusi pada performa prediksi yang lebih stabil dan akurat.

### **Step 5: Feature Engineering I - Time-Based Features (Rekayasa Fitur I - Fitur Berbasis Waktu)**

Langkah kelima dalam pipeline persiapan data ini adalah rekayasa fitur yang berfokus pada ekstraksi informasi berguna dari kolom tanggal (`Date`). Fitur berbasis waktu sangat penting dalam peramalan deret waktu karena dapat menangkap pola musiman (harian, mingguan, bulanan, tahunan) dan tren yang memengaruhi permintaan.

**Tujuan Langkah Ini:**

*   Mengekstrak komponen-komponen waktu dari kolom `Date`.
*   Membuat fitur-fitur numerik baru yang merepresentasikan posisi setiap observasi dalam siklus waktu (tahun, bulan, hari, hari dalam seminggu, dll.).
"""

# Create time-based features
master_df['year'] = master_df['Date'].dt.year
master_df['month'] = master_df['Date'].dt.month
master_df['day'] = master_df['Date'].dt.day
master_df['dayofweek'] = master_df['Date'].dt.dayofweek # Monday=0, Sunday=6
master_df['dayofyear'] = master_df['Date'].dt.dayofyear
master_df['weekofyear'] = master_df['Date'].dt.isocalendar().week.astype(int)
master_df['quarter'] = master_df['Date'].dt.quarter

print("Time-based features created.")
display(master_df[['Date', 'year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter']].head())

"""**Detail Kode dan Analisis:**

Kode di bagian ini menggunakan aksesori `.dt` dari tipe data datetime pandas untuk mengekstrak berbagai komponen waktu dari kolom `Date` yang sebelumnya telah dikonversi ke format datetime. Fitur-fitur baru yang dibuat meliputi:

1.  **`year`:** Mengekstrak tahun dari tanggal. Ini dapat membantu model mengidentifikasi tren tahunan atau perbedaan pola permintaan antar tahun.
    *   `master_df['year'] = master_df['Date'].dt.year`
2.  **`month`:** Mengekstrak bulan dari tanggal (1-12). Ini penting untuk menangkap pola musiman bulanan (misalnya, lonjakan permintaan saat liburan akhir tahun).
    *   `master_df['month'] = master_df['Date'].dt.month`
3.  **`day`:** Mengekstrak hari dalam bulan (1-31). Dapat menangkap pola yang terjadi pada tanggal-tanggal tertentu dalam sebulan (misalnya, awal atau akhir bulan).
    *   `master_df['day'] = master_df['Date'].dt.day`
4.  **`dayofweek`:** Mengekstrak hari dalam seminggu (0=Senin, 6=Minggu). Ini sangat penting untuk menangkap pola permintaan mingguan (misalnya, apakah penjualan lebih tinggi di akhir pekan).
    *   `master_df['dayofweek'] = master_df['Date'].dt.dayofweek`
5.  **`dayofyear`:** Mengekstrak hari dalam setahun (1-365 atau 366 untuk tahun kabisat). Dapat menangkap pola yang berulang pada hari yang sama setiap tahun, terlepas dari hari dalam seminggu.
    *   `master_df['dayofyear'] = master_df['Date'].dt.dayofyear`
6.  **`weekofyear`:** Mengekstrak nomor minggu dalam setahun. Mirip dengan `dayofyear`, ini dapat menangkap pola berdasarkan minggu tertentu dalam setahun. `.isocalendar().week` digunakan untuk penomoran minggu standar ISO. `.astype(int)` memastikan tipe data integer.
    *   `master_df['weekofyear'] = master_df['Date'].dt.isocalendar().week.astype(int)`
7.  **`quarter`:** Mengekstrak kuartal dalam setahun (1-4). Dapat menangkap pola permintaan musiman yang lebih luas antar kuartal.
    *   `master_df['quarter'] = master_df['Date'].dt.quarter`

Setelah fitur-fitur ini dibuat, pesan konfirmasi dicetak dan beberapa baris awal DataFrame ditampilkan, menyertakan kolom `Date` asli dan fitur-fitur waktu yang baru dibuat, untuk memverifikasi hasilnya.

Dengan menambahkan fitur-fitur berbasis waktu ini, model akan memiliki informasi tambahan untuk mengidentifikasi dan memanfaatkan pola-pola temporal dalam data permintaan, yang sangat penting untuk peramalan deret waktu yang akurat.

### **Step 6: Feature Engineering II - Lag Features (Rekayasa Fitur II - Fitur Lag)**

Langkah keenam dalam pipeline persiapan data berfokus pada pembuatan fitur lag. Fitur lag sangat penting dalam peramalan deret waktu karena menangkap nilai-nilai variabel target dari periode waktu sebelumnya. Ini secara efektif memberi model informasi tentang "apa yang terjadi di masa lalu", yang seringkali merupakan prediktor kuat untuk "apa yang akan terjadi di masa depan" dalam data time series.

**Tujuan Langkah Ini:**

*   Menciptakan fitur baru yang merepresentasikan nilai `Demand` dari *n* periode waktu sebelumnya (lag).
*   Memungkinkan model untuk belajar dari ketergantungan temporal data.
"""

# Create lag features for the 'Demand' column
master_df['demand_lag_7'] = master_df.groupby('Product ID')['Demand'].shift(7)
master_df['demand_lag_28'] = master_df.groupby('Product ID')['Demand'].shift(28)

# Handle resulting NaN values (e.g., fill with 0 or the mean of the lag feature)
# Filling with 0 as these are the initial periods where no lag data is available
master_df['demand_lag_7'] = master_df['demand_lag_7'].fillna(0)
master_df['demand_lag_28'] = master_df['demand_lag_28'].fillna(0)


print("Lag features created for 'Demand'.")
display(master_df[['Date', 'Product ID', 'Demand', 'demand_lag_7', 'demand_lag_28']].head(10))

"""**Detail Kode dan Analisis:**

Kode di bagian ini menggunakan fungsi `.shift()` pandas, yang sangat berguna untuk membuat fitur lag. Fungsi ini menggeser data dalam Series atau DataFrame berdasarkan jumlah periode yang ditentukan. Penggunaan `.groupby('Product ID')` sebelum `.shift()` memastikan bahwa operasi lag dilakukan secara terpisah untuk setiap produk. Ini penting karena kita ingin melihat lag permintaan *untuk produk yang sama*.

1.  **`master_df.groupby('Product ID')['Demand'].shift(7)`:**
    *   Mengelompokkan DataFrame (`master_df`) berdasarkan kolom `Product ID`. Operasi selanjutnya akan diterapkan secara independen pada setiap kelompok produk.
    *   Memilih kolom target `Demand` dalam setiap kelompok.
    *   Menerapkan `.shift(7)`. Ini menggeser nilai `Demand` ke bawah sebanyak 7 baris dalam setiap kelompok produk. Jadi, untuk baris pada tanggal T, nilai di kolom baru ini akan menjadi nilai `Demand` pada tanggal T-7 (7 hari sebelumnya). Hasilnya adalah fitur lag 7 hari, disimpan di kolom `demand_lag_7`.

2.  **`master_df.groupby('Product ID')['Demand'].shift(28)`:**
    *   Melakukan proses serupa untuk membuat fitur lag 28 hari (`demand_lag_28`), merepresentasikan nilai `Demand` 28 hari sebelumnya untuk setiap produk.

3.  **Penanganan Nilai `NaN` yang Dihasilkan:**
    *   Operasi `.shift()` akan menghasilkan nilai `NaN` untuk periode awal di setiap kelompok produk (misalnya, 7 baris pertama untuk `demand_lag_7`, 28 baris pertama untuk `demand_lag_28`), karena tidak ada data "masa lalu" yang cukup untuk menggeser.
    *   Kode menangani `NaN` ini dengan mengisi dengan nilai 0 menggunakan `.fillna(0)`. Ini adalah pendekatan yang umum untuk fitur lag awal di mana data historis tidak tersedia, mengasumsikan bahwa 'ketiadaan data lag' setara dengan nilai lag nol.

Setelah fitur lag dibuat dan nilai `NaN` diisi, pesan konfirmasi dicetak dan beberapa baris awal DataFrame ditampilkan, menyertakan kolom `Date`, `Product ID`, `Demand` asli, serta fitur lag yang baru dibuat, untuk memverifikasi hasilnya.

Dengan menambahkan fitur lag ini, model regresi akan memiliki informasi tentang tren dan pola permintaan terkini dari masa lalu, yang merupakan prediktor kunci dalam banyak skenario peramalan deret waktu.

### **Step 7: Feature Engineering III - Rolling Window Features (Rekayasa Fitur III - Fitur Jendela Bergulir)**

Langkah ketujuh dalam pipeline persiapan data berfokus pada pembuatan fitur jendela bergulir (rolling window features). Fitur ini membantu menangkap tren jangka pendek, pola musiman yang berulang, dan menghaluskan fluktuasi harian atau *noise* dalam data deret waktu. Fitur jendela bergulir dihitung berdasarkan nilai-nilai dalam rentang waktu (jendela) yang bergerak sepanjang deret waktu.

**Tujuan Langkah Ini:**

*   Menciptakan fitur baru yang merepresentasikan statistik ringkasan (seperti rata-rata) dari variabel target dalam jendela waktu tertentu di masa lalu.
*   Memberikan model informasi tentang perilaku permintaan rata-rata atau tren dalam periode waktu terkini.
"""

# Create rolling mean features for 'Demand'
master_df['demand_rolling_mean_7'] = master_df.groupby('Product ID')['Demand'].transform(lambda x: x.rolling(window=7).mean())
master_df['demand_rolling_mean_28'] = master_df.groupby('Product ID')['Demand'].transform(lambda x: x.rolling(window=28).mean())

# Handle resulting NaN values (initial periods) by filling with 0
# Filling with 0 as these are the initial periods where no rolling data is available
master_df['demand_rolling_mean_7'] = master_df['demand_rolling_mean_7'].fillna(0)
master_df['demand_rolling_mean_28'] = master_df['demand_rolling_mean_28'].fillna(0)


print("Rolling window features created for 'Demand'.")
display(master_df[['Date', 'Product ID', 'Demand', 'demand_rolling_mean_7', 'demand_rolling_mean_28']].head(30)) # Display more rows to see non-NaN values

"""**Detail Kode dan Analisis:**

Kode di bagian ini menggunakan kombinasi `.groupby()` dan `.rolling()` pandas untuk menghitung rata-rata bergulir dari kolom `Demand`.

1.  **`master_df.groupby('Product ID')['Demand']`:**
    *   Mirip dengan fitur lag, operasi ini mengelompokkan DataFrame (`master_df`) berdasarkan kolom `Product ID`. Ini memastikan bahwa rata-rata bergulir dihitung secara independen untuk setiap produk, menghindari pencampuran data antar produk yang dapat mendistorsi hasil.

2.  **`.transform(lambda x: x.rolling(window=7).mean())`:**
    *   Metode `.transform()` digunakan setelah `.groupby()`. Ini memungkinkan penerapan fungsi rolling (`.rolling()`) ke setiap kelompok produk secara independen, dan hasilnya akan memiliki indeks yang sama dengan DataFrame asli, sehingga mudah ditambahkan sebagai kolom baru.
    *   `x.rolling(window=7)`: Menerapkan fungsi rolling dengan ukuran jendela 7. Untuk setiap titik data, ini akan melihat 7 titik data sebelumnya (termasuk titik saat ini secara default, tetapi perilaku ini bisa diatur) dalam kelompok produk tersebut.
    *   `.mean()`: Menghitung rata-rata dari nilai-nilai `Demand` dalam jendela 7 hari tersebut. Hasilnya adalah rata-rata permintaan bergulir 7 hari, disimpan di kolom `demand_rolling_mean_7`.

3.  **`master_df.groupby('Product ID')['Demand'].transform(lambda x: x.rolling(window=28).mean())`:**
    *   Melakukan proses serupa untuk membuat rata-rata permintaan bergulir 28 hari (`demand_rolling_mean_28`), merepresentasikan rata-rata permintaan selama 28 hari sebelumnya untuk setiap produk. Rata-rata 28 hari sering digunakan untuk menangkap pola bulanan atau tren jangka menengah.

4.  **Penanganan Nilai `NaN` yang Dihasilkan:**
    *   Operasi `.rolling()` juga akan menghasilkan nilai `NaN` untuk periode awal di setiap kelompok produk karena tidak ada cukup data historis untuk mengisi jendela rolling. Misalnya, untuk rata-rata 7 hari, 6 baris pertama di setiap kelompok produk akan memiliki nilai `NaN`.
    *   Kode menangani `NaN` ini dengan mengisi dengan nilai 0 menggunakan `.fillna(0)`. Mirip dengan fitur lag, ini adalah pendekatan yang umum untuk periode awal di mana data rolling tidak tersedia, mengasumsikan 'ketiadaan data rolling' setara dengan rata-rata bergulir nol pada periode tersebut.

Setelah fitur jendela bergulir dibuat dan nilai `NaN` diisi, pesan konfirmasi dicetak dan beberapa baris awal DataFrame ditampilkan untuk memverifikasi hasilnya. Menampilkan lebih banyak baris (misalnya 30, seperti di contoh kode) berguna untuk melihat kapan nilai non-NaN mulai muncul (setelah 6 baris untuk jendela 7 hari, dan 27 baris untuk jendela 28 hari, ditambah offset jika menggunakan `.shift()` sebelum `.rolling()`).

Dengan menambahkan fitur jendela bergulir ini, model akan memiliki informasi tentang tren permintaan terkini dan pola rata-rata dalam periode waktu tertentu, yang dapat membantu meningkatkan akurasi peramalan dengan menangkap dinamika deret waktu yang lebih halus dibandingkan hanya fitur lag.

Fitur jendela bergulir, seperti rata-rata bergulir, membantu menghaluskan *noise* dan menangkap tren jangka pendek dalam data.

### **Step 8: Categorical Feature Encoding (Pengkodean Fitur Kategorikal)**

Langkah kedelapan dalam pipeline persiapan data ini berfokus pada transformasi fitur kategorikal menjadi format numerik yang dapat dipahami oleh algoritma machine learning. Metode yang umum dan efektif untuk ini adalah One-Hot Encoding.

**Tujuan Langkah Ini:**

*   Mengidentifikasi kolom-kolom dalam DataFrame yang berisi data kategorikal.
*   Mengubah setiap kategori unik dalam kolom-kolom tersebut menjadi kolom biner (0 atau 1).
*   Menyiapkan data kategorikal untuk dimasukkan ke dalam model machine learning.
"""

# Identify categorical columns (excluding 'Date' and 'Product ID' which are not features for encoding here)
categorical_features_for_encoding = master_df.select_dtypes(include='object').columns.tolist()
# Include 'Store ID' for one-hot encoding
# Note: 'Category', 'Region', 'Weather Condition', 'Seasonality', 'SUPPLIER', 'ITEM TYPE' are good candidates

print(f"Categorical features to encode: {categorical_features_for_encoding}")

# Apply One-Hot Encoding
master_df = pd.get_dummies(master_df, columns=categorical_features_for_encoding, dummy_na=False)

print("Categorical features encoded using One-Hot Encoding.")
display(master_df.head())

"""**Detail Kode dan Analisis:**

1.  **Identifikasi Kolom Kategorikal (`master_df.select_dtypes(include='object').columns.tolist()`):**
    *   Kode ini pertama-tama mengidentifikasi semua kolom dalam `master_df` yang memiliki tipe data 'object'. Kolom bertipe 'object' biasanya merepresentasikan data string atau kategorikal.
    *   Kolom 'Date' dan 'Product ID' secara eksplisit dikecualikan dari daftar untuk encoding ini, karena 'Date' akan diekstraksi fiturnya secara terpisah (Langkah 5) dan 'Product ID' mungkin diperlakukan sebagai pengidentifikasi unik atau di-encode dengan cara lain jika diperlukan (meskipun dalam kode yang ada, 'Product ID' juga termasuk dalam daftar yang akan di-encode karena tipenya 'object'). Kolom-kolom seperti 'Store ID', 'Category', 'Region', 'Weather Condition', 'Seasonality', serta kolom dari merge seperti 'SUPPLIER' dan 'ITEM TYPE' adalah kandidat utama untuk One-Hot Encoding.
    *   Daftar nama kolom kategorikal yang akan di-encode dicetak untuk verifikasi.

2.  **Penerapan One-Hot Encoding (`pd.get_dummies()`):**
    *   Fungsi `pd.get_dummies()` dari pandas digunakan untuk melakukan One-Hot Encoding pada kolom-kolom yang telah diidentifikasi.
    *   `columns=categorical_features_for_encoding`: Argumen ini menentukan kolom mana yang akan di-encode.
    *   `dummy_na=False`: Argumen ini memastikan bahwa nilai `NaN` (jika ada) *tidak* diubah menjadi kolom terpisah. Karena kita sudah menangani nilai missing di Langkah 1 (mengisi dengan "Unknown" atau nilai lain), argumen ini memastikan `NaN` yang tersisa (jika ada) diabaikan oleh proses encoding.
    *   Fungsi ini bekerja dengan membuat kolom biner baru untuk setiap nilai unik dalam kolom kategorikal asli. Misalnya, jika kolom 'Region' memiliki nilai 'North', 'South', 'East', dan 'West', One-Hot Encoding akan membuat empat kolom baru: 'Region\_North', 'Region\_South', 'Region\_East', dan 'Region\_West'. Untuk setiap baris, salah satu dari kolom biner ini akan bernilai 1 (sesuai dengan kategori baris tersebut), dan yang lainnya 0.
    *   DataFrame asli `master_df` ditimpa dengan hasil encoding, yang mencakup kolom-kolom biner baru menggantikan kolom kategorikal asli yang di-encode.

Setelah One-Hot Encoding diterapkan, pesan konfirmasi dicetak dan beberapa baris awal `master_df` ditampilkan. Output `head()` menunjukkan kolom-kolom biner baru yang ditambahkan ke DataFrame.

**Pertimbangan Profesional:**

*   **Jumlah Kategori:** One-Hot Encoding dapat menghasilkan banyak kolom baru jika ada kolom kategorikal dengan jumlah nilai unik yang sangat tinggi (kardinalitas tinggi). Ini bisa meningkatkan dimensi DataFrame secara signifikan, yang dapat memengaruhi kinerja dan waktu pelatihan model. Untuk kolom dengan kardinalitas tinggi (seperti 'SUPPLIER' atau 'Product ID' jika ada ribuan produk unik), teknik encoding alternatif seperti Target Encoding atau Frequency Encoding mungkin lebih efisien. Dalam kasus ini, 'Product ID' memiliki 20 nilai unik, yang masih bisa dikelola dengan One-Hot Encoding. 'SUPPLIER' dan 'ITEM TYPE' memiliki banyak nilai 'Unknown' setelah merge, tetapi jika data dari `df2` berhasil digabung, mereka juga bisa memiliki kardinalitas tinggi.
*   **Multikolinieritas:** One-Hot Encoding menciptakan kelompok kolom biner yang bersifat saling eksklusif (untuk setiap baris, hanya satu kolom dalam kelompok yang bernilai 1). Dalam model linier (seperti Linear Regression), ini dapat menyebabkan masalah multikolinieritas sempurna jika tidak satu kategori di-drop. Namun, untuk model berbasis pohon seperti XGBoost, ini biasanya bukan masalah. Argumen `drop_first=True` di `pd.get_dummies` dapat digunakan untuk menghilangkan satu kolom dari setiap kelompok kategori dan menghindari multikolinieritas.

Langkah pengkodean ini sangat penting untuk mempersiapkan data kategorikal agar dapat digunakan sebagai input untuk sebagian besar algoritma machine learning, memastikan bahwa informasi kategorikal tetap dipertahankan dalam format numerik yang sesuai.

### **Step 9: Feature Scaling (Penskalaan Fitur)**

Penskalaan fitur (Feature Scaling) adalah langkah pra-pemrosesan data yang penting untuk sebagian besar algoritma machine learning, terutama yang berbasis jarak (seperti K-Nearest Neighbors, Support Vector Machines) atau yang menggunakan metode berbasis gradien (seperti regresi linier atau jaringan saraf). Tujuannya adalah untuk menstandardisasi atau menormalkan rentang nilai dari fitur-fitur numerik.

**Tujuan Langkah Ini:**

*   Memastikan bahwa tidak ada satu fitur pun yang mendominasi proses pelatihan model hanya karena rentang nilainya lebih besar.
*   Mempercepat konvergensi algoritma berbasis gradien.
*   Meningkatkan kinerja model yang sensitif terhadap skala fitur.
"""

# Identify numerical features to scale
# Exclude 'Date', 'Product ID', the target variable 'Demand', and the newly created binary/boolean columns from one-hot encoding
numerical_features_to_scale = X.select_dtypes(include=np.number).columns.tolist()

# Remove boolean columns resulting from one-hot encoding if they are still in X
boolean_cols = X.select_dtypes(include='bool').columns.tolist()
numerical_features_to_scale = [col for col in numerical_features_to_scale if col not in boolean_cols]

# Exclude lag and rolling features as they are already derived from scaled data or handled
lag_rolling_features = ['demand_lag_7', 'demand_lag_28', 'demand_rolling_mean_7', 'demand_rolling_mean_28', 'Units Sold_capped'] # Added 'Units Sold_capped'
numerical_features_to_scale = [col for col in numerical_features_to_scale if col not in lag_rolling_features]

print(f"Numerical features to scale: {numerical_features_to_scale}")

# Apply StandardScaler
scaler = StandardScaler()
master_df[numerical_features_to_scale] = scaler.fit_transform(master_df[numerical_features_to_scale])

print("Numerical features scaled using StandardScaler.")
display(master_df.head())

"""**Detail Kode dan Analisis:**

1.  **Identifikasi Fitur Numerik untuk Skalasi (`X.select_dtypes(include=np.number).columns.tolist()` dan Filter):**
    *   Kode ini pertama-tama mengidentifikasi semua kolom dalam DataFrame fitur (`X`) yang memiliki tipe data numerik. Penting untuk melakukan ini setelah One-Hot Encoding, karena beberapa kolom numerik asli mungkin telah diubah atau dihapus.
    *   Beberapa kolom numerik dikecualikan dari proses penskalaan:
        *   Kolom boolean (hasil dari One-Hot Encoding) dikecualikan karena sudah dalam format 0/1 yang terstandardisasi.
        *   Kolom pengidentifikasi seperti 'Product ID' (jika masih ada dalam bentuk numerik) biasanya tidak diskalakan.
        *   Kolom fitur berbasis waktu yang diekstraksi sebagai integer (seperti `year`, `month`, `dayofweek`, dll.) juga sering dikecualikan dari penskalaan standar jika model dapat menafsirkan nilai integer tersebut secara langsung (misalnya, model berbasis pohon).
        *   Fitur lag dan rolling window yang dibuat dari variabel target mungkin sudah memiliki skala yang mirip dengan target, atau penanganannya bisa dipertimbangkan secara terpisah. Dalam kode ini, fitur lag dan rolling window serta kolom `_capped` ditambahkan ke daftar pengecualian.
    *   Daftar akhir fitur numerik yang akan diskalakan dicetak untuk verifikasi.

2.  **Penerapan StandardScaler:**
    *   `scaler = StandardScaler()`: Membuat instance dari `StandardScaler`. StandardScaler menstandardisasi fitur dengan menghapus rata-rata dan menskalakan ke varian unit. Rumusnya adalah z = (x - u) / s, di mana u adalah rata-rata sampel dan s adalah standar deviasi sampel.
    *   `master_df[numerical_features_to_scale] = scaler.fit_transform(master_df[numerical_features_to_scale])`: Metode `fit_transform()` digunakan pada DataFrame `master_df` yang difilter hanya untuk kolom-kolom yang akan diskalakan.
        *   `fit()` menghitung parameter (rata-rata dan standar deviasi) dari data pelatihan (dalam kasus ini, seluruh `master_df` sebelum split, idealnya ini dilakukan *setelah* split menggunakan hanya data pelatihan untuk mencegah *data leakage*).
        *   `transform()` kemudian menerapkan penskalaan menggunakan parameter yang dihitung.
    *   Hasil penskalaan menimpa kolom asli dalam `master_df`.

Setelah penskalaan diterapkan, pesan konfirmasi dicetak dan beberapa baris awal `master_df` ditampilkan. Anda akan melihat bahwa nilai-nilai dalam kolom yang diskalakan sekarang berada di sekitar 0 dengan standar deviasi sekitar 1.

**Catatan Penting (Pertimbangan Profesional):**

Idealnya, proses `fit()` pada scaler (menghitung rata-rata dan standar deviasi) hanya boleh dilakukan pada **data pelatihan (training data)**. Kemudian, scaler yang sudah 'dilatih' tersebut (`scaler`) digunakan untuk `transform()` baik data pelatihan maupun data validasi/pengujian. Melakukan `fit_transform()` pada seluruh `master_df` sebelum membagi data menjadi pelatihan dan validasi dapat menyebabkan *data leakage*, di mana informasi tentang distribusi data validasi 'bocor' ke dalam proses penskalaan data pelatihan. Ini dapat membuat model terlihat berkinerja lebih baik dari yang sebenarnya saat dievaluasi pada data validasi.

Namun, dalam konteks notebook ini yang menunjukkan langkah-langkah pipeline secara berurutan, kode yang ada menunjukkan cara penerapan scaler secara umum. Dalam implementasi yang sebenarnya untuk peramalan deret waktu dengan split data kronologis, penskalaan harus dilakukan **setelah** data dibagi, dengan scaler dilatih hanya pada `X_train` dan kemudian digunakan untuk mentransformasi `X_train` dan `X_val`.

Langkah penskalaan ini memastikan bahwa semua fitur numerik memiliki kontribusi yang setara terhadap jarak atau perhitungan gradien dalam model, yang krusial untuk kinerja model yang optimal.

### **Step 10: Time-Based Data Splitting (Pembagian Data Berbasis Waktu)**

Langkah kesepuluh dan terakhir dalam pipeline persiapan data ini adalah membagi dataset utama (`master_df`) menjadi set pelatihan (training set) dan set validasi (validation set). Untuk data deret waktu (time series), metode pembagian ini sangat krusial dan harus dilakukan secara kronologis.

**Tujuan Langkah Ini:**

*   Membagi data menjadi set pelatihan dan validasi berdasarkan batas waktu (cutoff date).
*   Memastikan bahwa data validasi hanya berisi observasi yang terjadi *setelah* data pelatihan.
*   Menghindari *data leakage* temporal, yang merupakan sumber kesalahan umum dalam proyek time series.
"""

# Define the chronological cutoff date for splitting
# Using a date in early 2023 as an example cutoff
cutoff_date = pd.to_datetime('2023-01-01')

# Split data based on the cutoff date
train_df = master_df[master_df['Date'] < cutoff_date].copy()
val_df = master_df[master_df['Date'] >= cutoff_date].copy()

# Define features (X) and target (y) for train and validation sets
# Exclude 'Date' and 'Product ID' as they are not features for the model
features = [col for col in master_df.columns if col not in ['Date', 'Product ID', target_variable]]

X_train = train_df[features]
y_train = train_df[target_variable]

X_val = val_df[features]
y_val = val_df[target_variable]


print(f"Data split into training and validation sets based on cutoff date: {cutoff_date}")
print(f"Training set shape: {X_train.shape}")
print(f"Validation set shape: {X_val.shape}")

"""**Detail Kode dan Analisis:**

1.  **Definisi Batas Waktu (`cutoff_date`):**
    *   Sebuah tanggal ditentukan sebagai batas (`cutoff_date`) untuk membagi data. Dalam kode ini, tanggal `2023-01-01` digunakan sebagai contoh batas waktu. Semua data dengan tanggal *sebelum* batas ini akan masuk ke set pelatihan, dan data dengan tanggal *sama dengan atau setelah* batas ini akan masuk ke set validasi.
    *   Menggunakan `pd.to_datetime()` memastikan bahwa `cutoff_date` adalah objek datetime yang dapat dibandingkan dengan kolom 'Date' di DataFrame.

2.  **Pembagian Data Berdasarkan Tanggal:**
    *   `train_df = master_df[master_df['Date'] < cutoff_date].copy()`: Baris ini memfilter `master_df` untuk memilih semua baris di mana kolom 'Date' lebih awal dari `cutoff_date`. Hasilnya disimpan di `train_df`. `.copy()` digunakan untuk memastikan bahwa `train_df` adalah salinan independen dari `master_df`, menghindari peringatan SettingWithCopyWarning jika modifikasi dilakukan pada `train_df` nanti.
    *   `val_df = master_df[master_df['Date'] >= cutoff_date].copy()`: Baris ini memfilter `master_df` untuk memilih semua baris di mana kolom 'Date' sama dengan atau lebih lambat dari `cutoff_date`. Hasilnya disimpan di `val_df`. `.copy()` juga digunakan di sini.

3.  **Definisi Fitur (X) dan Target (y) untuk Set Pelatihan dan Validasi:**
    *   Setelah membagi DataFrame berdasarkan waktu, langkah selanjutnya adalah memisahkan fitur (variabel independen) dari variabel target (`Demand`) untuk kedua set (`train_df` dan `val_df`).
    *   `features = [col for col in master_df.columns if col not in ['Date', 'Product ID', target_variable]]`: Sebuah list `features` dibuat yang berisi nama semua kolom di `master_df` *kecuali* kolom 'Date', 'Product ID' (keduanya adalah pengidentifikasi dan tidak digunakan sebagai fitur langsung oleh model), dan variabel target (`Demand`). Fitur-fitur yang dibuat di langkah-langkah rekayasa fitur sebelumnya akan otomatis masuk ke dalam list ini.
    *   `X_train = train_df[features]`: DataFrame `X_train` dibuat, berisi hanya kolom-kolom fitur dari `train_df`.
    *   `y_train = train_df[target_variable]`: Series `y_train` dibuat, berisi hanya kolom target (`Demand`) dari `train_df`.
    *   `X_val = val_df[features]`: DataFrame `X_val` dibuat, berisi hanya kolom-kolom fitur dari `val_df`.
    *   `y_val = val_df[target_variable]`: Series `y_val` dibuat, berisi hanya kolom target (`Demand`) dari `val_df`.

4.  **Verifikasi Ukuran Set Data:**
    *   Mencetak bentuk (`shape`) dari `X_train`, `y_train`, `X_val`, dan `y_val` untuk mengkonfirmasi bahwa pembagian telah terjadi dan set data memiliki jumlah baris dan kolom yang diharapkan.

**Pentingnya Pembagian Kronologis:**

Dalam peramalan deret waktu, tujuan model adalah memprediksi nilai *masa depan* berdasarkan data *masa lalu*. Jika data dibagi secara acak (seperti pada `train_test_split` standar untuk data non-time series), data dari periode waktu *setelah* periode validasi bisa saja bocor ke dalam set pelatihan. Ini akan memberikan model informasi tentang masa depan yang tidak akan tersedia saat model digunakan dalam skenario nyata, menghasilkan evaluasi performa yang terlalu optimis (data leakage). Pembagian kronologis secara ketat memastikan bahwa model hanya dilatih pada data historis dan dievaluasi pada data yang benar-benar 'tidak terlihat' dari masa depan.

Dengan menyelesaikan langkah 10 ini, data telah berhasil dibersihkan, direkayasa fiturnya, diskalakan, dan dibagi dengan benar, siap untuk digunakan dalam proses pelatihan dan evaluasi model di bab selanjutnya.

## **Chapter 4: Post-Preparation Data Summary (Ringkasan Data Pasca-Persiapan)**

Setelah menyelesaikan 10 langkah komprehensif dalam pipeline persiapan data, dataset `master_df` kini telah dibersihkan, direkayasa fiturnya, dan diskalakan, siap untuk digunakan dalam proses pelatihan model. Bab ini menyajikan ringkasan dari DataFrame akhir ini, memastikan bahwa semua langkah pra-pemrosesan telah diterapkan dengan benar dan data berada dalam format yang siap model.

### **4.1. Final Data Snapshot (Gambaran Data Akhir)**

Melihat beberapa baris pertama dari DataFrame yang telah diproses.
"""

print("Head of the final master_df after 10 preparation steps:")
display(master_df.head())

"""### **4.2. Final Data Structure and Types (Struktur dan Tipe Data Akhir)**

Memeriksa struktur DataFrame akhir, termasuk jumlah kolom, jumlah non-null, dan tipe data.
"""

print("\nInfo of the final master_df:")
master_df.info()

"""### **4.3. Final Summary Statistics (Ringkasan Statistik Akhir)**

Menampilkan statistik deskriptif untuk kolom numerik dan non-numerik di DataFrame akhir.
"""

print("\nSummary statistics of the final master_df:")
display(master_df.describe(include='all'))

display(master_df.describe())

"""### **4.4. Final Missing Values and Duplicates Check (Pemeriksaan Akhir Nilai Hilang dan Duplikat)**

Memastikan tidak ada lagi nilai yang hilang yang perlu ditangani dan tidak ada baris duplikat.
"""

print("\nMissing values in the final master_df:")
print(master_df.isnull().sum().to_markdown(numalign="left", stralign="left"))

print("\nDuplicate rows in the final master_df:")
print(master_df.duplicated().sum())

"""### **4.5. Saving the Prepared Dataset (Menyimpan Dataset yang Telah Disiapkan)**

Menyimpan DataFrame yang telah diproses ke file CSV untuk penggunaan di masa mendatang atau untuk dibagikan.
"""

# Save the prepared master_df to a CSV file
master_df.to_csv('master_df_prepared.csv', index=False)
print("\nPrepared master_df saved to 'master_df_prepared.csv'")

"""### **4.6. Potential Additional Features and Improvements (Potensi Fitur Tambahan dan Peningkatan)**

Meskipun 10 langkah persiapan data telah mencakup proses yang komprehensif, ada beberapa area atau fitur tambahan yang dapat dieksplorasi untuk meningkatkan akurasi model peramalan permintaan:

*   **Interaksi Fitur:** Buat fitur interaksi antara fitur kategorikal (seperti Kategori Produk atau ID Toko) dan fitur berbasis waktu (seperti bulan atau hari dalam seminggu). Ini dapat menangkap pola permintaan unik untuk produk/toko tertentu pada waktu-waktu tertentu.
    *   *Contoh:* Interaksi antara `Category_Electronics` dan `month` untuk melihat apakah permintaan elektronik memiliki pola musiman yang kuat di bulan-bulan tertentu.
*   **Fitur Kalender Lanjutan:** Tambahkan fitur yang lebih spesifik terkait kalender, seperti:
    *   `days_until_next_holiday`: Jarak hari hingga hari libur besar berikutnya.
    *   `days_since_last_holiday`: Jarak hari sejak hari libur besar terakhir.
    *   `is_promo_week`: Indikator apakah minggu tersebut memiliki promosi.
*   **Fitur Berbasis Toko dan Produk:**
    *   Agregasi data di tingkat toko atau produk untuk membuat fitur seperti rata-rata penjualan historis per toko atau jumlah produk yang tersedia di toko tertentu.
    *   Fitur demografi atau lokasi toko (jika data tersedia).
*   **Analisis Sentimen Eksternal:** Jika memungkinkan, integrasikan data sentimen dari media sosial atau berita terkait produk atau industri ritel.
*   **Kondisi Ekonomi yang Lebih Spesifik:** Jika 'economic\_index' dari `df3` terlalu umum, coba cari atau buat indeks ekonomi yang lebih spesifik untuk wilayah atau segmen pasar ritel yang relevan.
*   **Penanganan Missing Values dari Merge:** Seperti yang dibahas sebelumnya, investigasi lebih lanjut mengapa banyak nilai hilang setelah penggabungan dari `df2` dan `df3` sangat penting. Jika data tersebut memang relevan, strategi penggabungan atau imputasi yang lebih canggih mungkin diperlukan.
*   **Normalisasi pada Fitur Hasil Capping:** Setelah melakukan capping, fitur-fitur tersebut mungkin masih memiliki rentang yang bervariasi. Melakukan penskalaan (seperti StandardScaler atau MinMaxScaler) pada fitur-fitur `_capped` ini sebelum dimasukkan ke model dapat bermanfaat.

Menambahkan fitur-fitur ini dapat membantu model menangkap pola permintaan yang lebih kompleks dan spesifik, berpotensi meningkatkan kinerja peramalan.

## **Chapter 5: Conclusion**

This project successfully implemented a comprehensive data pipeline for retail demand forecasting, covering data ingestion, merging from multiple sources, and a detailed 10-step data preparation process. The preparation pipeline included essential steps such as data cleaning, type conversion, exploratory data analysis (EDA), outlier treatment, and robust feature engineering (time-based, lag, and rolling window features). Categorical features were effectively encoded, and numerical features were scaled to ensure data readiness for machine learning models. Finally, the data was split chronologically, a crucial step for time-series forecasting to prevent temporal data leakage.

The resulting prepared dataset is now clean, well-structured, and enriched with relevant features, making it suitable for the next stage of the machine learning pipeline: model training, evaluation, and deployment for accurate daily product demand forecasting. This comprehensive preparation lays a strong foundation for building a robust and reliable forecasting solution to support inventory management and optimize retail operations.

## **Chapter 5: Kesimpulan**

Proyek ini berhasil menerapkan *pipeline* data yang komprehensif untuk peramalan permintaan ritel. Prosesnya meliputi penyerapan dan penggabungan data dari berbagai sumber, serta 10 langkah persiapan data yang detail. Tahap persiapan ini mencakup langkah-langkah penting seperti pembersihan data, konversi tipe data, analisis data eksplorasi (EDA), penanganan *outlier*, dan rekayasa fitur yang kuat (fitur berbasis waktu, *lag*, dan *rolling window*). Fitur-fitur kategorikal di-*encode* dengan efektif, dan fitur numerik di-*scale* untuk memastikan data siap digunakan oleh model *machine learning*. Terakhir, data dibagi secara kronologis, langkah krusial untuk peramalan deret waktu guna mencegah kebocoran data temporal.

*Dataset* yang dihasilkan dari proses persiapan ini kini bersih, terstruktur dengan baik, dan diperkaya dengan fitur-fitur yang relevan. Ini menjadikannya siap untuk tahap selanjutnya dalam *pipeline machine learning*: pelatihan model, evaluasi, dan implementasi untuk peramalan permintaan produk harian yang akurat. Persiapan komprehensif ini meletakkan fondasi yang kuat untuk membangun solusi peramalan yang tangguh dan dapat diandalkan guna mendukung manajemen inventaris dan mengoptimalkan operasional ritel.

## **Kerangka Laporan Proyek: Retail  Demand Forecasting: Pendekatan Pipeline Machine Learning**

Berikut adalah kerangka laporan profesional yang dapat Anda gunakan untuk menyusun laporan proyek ini, mencakup semua tahapan hingga Ringkasan Data Pasca-Persiapan (Chapter 4), sesuai instruksi terbaru Anda:

### **1. Pendahuluan (Introduction)**
* **Latar Belakang Masalah:** Jelaskan pentingnya peramalan permintaan yang akurat dalam industri ritel (manajemen inventaris, pengurangan biaya, kepuasan pelanggan).
* **Tujuan Proyek:** Nyatakan dengan jelas tujuan proyek dalam konteks tugas ini, yaitu untuk melakukan eksplorasi data mendalam dan menyiapkan data secara komprehensif untuk peramalan permintaan harian produk.
* **Ruang Lingkup Proyek:** Sebutkan bahwa proyek ini berfokus pada tahap awal pipeline ML, yaitu penggabungan, pembersihan, dan rekayasa fitur data, hingga data siap untuk pemodelan. (Secara eksplisit nyatakan bahwa tahap pemodelan tidak termasuk dalam laporan ini).
* **Struktur Laporan:** Berikan gambaran singkat tentang bagian-bagian selanjutnya dalam laporan.

### **2. Pemahaman Data Awal (Initial Data Understanding)**
* **Sumber Data:** Jelaskan secara rinci ketiga dataset yang digunakan (Retail Store Inventory and Demand Forecasting, Retail Sales Data with Seasonal Trends & Marketing, Strategic Supply Chain Demand Forecasting Dataset) dan tautannya.
* **Deskripsi Data Awal:** Berikan gambaran umum tentang konten masing-masing dataset, fitur-fitur kunci, dan temuan awal dari `.head()`, `.info()`, dan `.describe()` untuk setiap dataset mentah.
* **Analisis Awal Nilai Hilang dan Duplikat:** Sajikan temuan awal mengenai nilai hilang dan baris duplikat di masing-masing dataset mentah. Diskusikan implikasinya.

### **3. Data Ingestion and Merging (Penyerapan dan Penggabungan Data)**
* **Strategi Penggabungan Data:** Jelaskan mengapa ketiga dataset ini digabungkan, kunci penggabungan yang digunakan (`Product ID`, `Date`), dan metode penggabungan (`how='left'`). Diskusikan hasil awal penggabungan (misalnya, munculnya banyak nilai hilang).

### **4. Metodologi: Pipeline Persiapan Data 10 Langkah (Methodology: 10-Step Data Preparation Pipeline)**
* Jelaskan secara umum bahwa data melalui pipeline persiapan 10 langkah, dimulai dari pembersihan data hingga rekayasa fitur dan penskalaan.
* **4.1. Data Cleaning and Type Conversion (Pembersihan Data dan Konversi Tipe):** Jelaskan secara rinci proses pembersihan awal `master_df` setelah penggabungan. Fokus pada:
    * Ringkasan awal `master_df` (`info()`, `head()`).
    * Identifikasi nilai hilang setelah merge.
    * Strategi penanganan nilai hilang (pengisian "Unknown", pengisian 0/median) dan justifikasinya. Sebutkan penanganan kolom `future_demand` (dihapus dan alasannya).
    * Konversi kolom `Date` ke format datetime.
    * Pemeriksaan akhir duplikat.
* **4.2. Column Consolidation and Selection (Konsolidasi dan Pemilihan Kolom):** Jelaskan penghapusan kolom berlebihan/redundant dan pemisahan fitur (X) dan target (y).
* **4.3. Exploratory Data Analysis (EDA):** Jelaskan tujuan EDA. Sajikan dan interpretasikan visualisasi dan temuan kunci dari EDA:
    * Tren permintaan dari waktu ke waktu (dengan plot).
    * Distribusi variabel target (`Demand`) dan outlier (dengan statistik dan plot).
    * Hubungan fitur kunci vs. `Demand` (contoh: harga, diskon, promosi dengan plot).
    * Distribusi fitur numerik dan kategorikal (dengan contoh plot).
    * Hasil analisis korelasi dan wawasannya.
* **4.4. Outlier Detection and Treatment (Deteksi dan Perlakuan Outlier):** Jelaskan identifikasi outlier pada fitur numerik (dengan box plot). Jelaskan metode capping IQR yang diterapkan pada fitur dan alasannya. Jelaskan mengapa target (`Demand`) tidak dikenai perlakuan outlier. Sajikan contoh data sebelum/sesudah capping.
* **4.5. Feature Engineering I - Time-Based Features (Rekayasa Fitur I - Fitur Berbasis Waktu):** Jelaskan fitur berbasis waktu yang diekstraksi (tahun, bulan, hari, hari dalam seminggu, hari dalam setahun, minggu dalam setahun, kuartal).
* **4.6. Feature Engineering II - Lag Features (Rekayasa Fitur II - Fitur Lag):** Jelaskan pembuatan fitur lag dari `Demand` (`demand_lag_7`, `demand_lag_28`) menggunakan `groupby` dan `shift`. Jelaskan penanganan nilai `NaN`.
* **4.7. Feature Engineering III - Rolling Window Features (Rekayasa Fitur III - Fitur Jendela Bergulir):** Jelaskan pembuatan fitur rata-rata bergulir dari `Demand` (`demand_rolling_mean_7`, `demand_rolling_mean_28`) menggunakan `groupby` dan `rolling`. Jelaskan penanganan nilai `NaN`.
* **4.8. Categorical Feature Encoding (Pengkodean Fitur Kategorikal):** Jelaskan identifikasi kolom kategorikal dan penerapan One-Hot Encoding menggunakan `pd.get_dummies()`. Sebutkan pertimbangan terkait kardinalitas tinggi jika ada.
* **4.9. Feature Scaling (Penskalaan Fitur):** Jelaskan identifikasi fitur numerik untuk penskalaan dan penerapan StandardScaler. **Sebutkan secara eksplisit bahwa dalam skenario time series nyata, penskalaan idealnya dilakukan *setelah* data split, dilatih hanya pada data training untuk menghindari leakage, meskipun dalam notebook ini ditunjukkan secara umum sebelum split.**
* **4.10. Time-Based Data Splitting (Pembagian Data Berbasis Waktu):** Jelaskan metode pembagian data kronologis (`train_df`, `val_df`) berdasarkan `cutoff_date` dan mengapa ini penting untuk data time series guna menghindari data leakage temporal. Sebutkan ukuran akhir `X_train`, `y_train`, `X_val`, `y_val`.

### **5. Ringkasan Data Pasca-Persiapan (Post-Preparation Data Summary)**
* **Gambaran Data Akhir:** Tampilkan beberapa baris pertama dari `master_df` yang telah selesai diproses (`.head()`).
* **Struktur dan Tipe Data Akhir:** Sajikan ringkasan struktur data akhir (`.info()`) dan diskusikan perubahan jumlah kolom, tipe data (datetime, float hasil scaling, bool hasil encoding), dan konfirmasi tidak ada nilai non-null di kolom yang telah ditangani.
* **Ringkasan Statistik Akhir:** Tampilkan statistik deskriptif akhir (`.describe(include='all')`) dan diskusikan bagaimana distribusi fitur numerik berubah setelah penskalaan (rata-rata mendekati 0, std mendekati 1).
* **Pemeriksaan Akhir Nilai Hilang dan Duplikat:** Sajikan output `.isnull().sum()` dan `.duplicated().sum()` terakhir untuk mengkonfirmasi bahwa tidak ada lagi nilai hilang (di kolom yang ditangani) dan duplikat.
* **Penyimpanan Dataset:** Sebutkan bahwa dataset yang telah disiapkan disimpan ke file CSV.
* **Potensi Fitur Tambahan dan Peningkatan (Opsional, sebagai bagian dari diskusi temuan atau future work):** Jika diminta untuk mencakup saran peningkatan, bisa dimasukkan di sini atau di bagian terpisah jika laporan diperluas nanti. (Jika laporan hanya sampai sini, ini bisa menjadi penutup untuk bagian ini).

### **6. Kesimpulan (Conclusion)**
* Ringkas proses persiapan data yang telah dilakukan dan sebutkan bahwa data kini siap untuk tahap pemodelan peramalan permintaan.

### **7. Referensi (References)**
* Cantumkan sumber data dan library utama yang digunakan.

Kerangka ini memberikan struktur yang detail dan profesional hingga tahap persiapan data. Anda dapat mengisinya dengan output spesifik dari notebook Anda dan narasi yang menjelaskan setiap langkah dan temuan. Semoga berhasil!

## **Kerangka Presentasi (PPT): Retail  Demand Forecasting: Pendekatan Pipeline Machine Learning**

Berikut adalah kerangka presentasi profesional yang dapat Anda gunakan untuk menyusun slide presentasi, mencakup semua tahapan hingga Ringkasan Data Pasca-Persiapan (sesuai dengan kerangka laporan yang telah dibuat):

---

### **Slide 1: Halaman Judul**
* **Judul:** Demand Forecasting for Retail: A Machine Learning Pipeline Approach
* **Sub-judul:** Tahap Persiapan Data Komprehensif
* **Nama Anda / Kelompok:** [Nama Anda / Nama Kelompok]
* **Nama Mata Kuliah:** Model Development Engineering
* **Tanggal:** [Tanggal Presentasi]

---

### **Slide 2: Pendahuluan (Introduction)**
* **Latar Belakang:**
    * Pentingnya Peramalan Permintaan di Ritel (Manajemen Inventaris, Pengurangan Biaya)
    * Tantangan dalam Peramalan Permintaan
* **Tujuan Proyek:**
    * Mengembangkan pipeline ML untuk peramalan permintaan harian produk.
    * Fokus pada tahap eksplorasi data dan persiapan data komprehensif.
* **Ruang Lingkup Presentasi:**
    * Hanya mencakup tahap persiapan data hingga data siap untuk pemodelan.
    * Tidak mencakup detail pemodelan dan evaluasi model.

---

### **Slide 3: Pemahaman Data Awal (Initial Data Understanding)**
* **Sumber Data:**
    * Dataset 1: Retail Store Inventory and Demand Forecasting (Data Transaksional Inti)
    * Dataset 2: Retail Sales Data with Seasonal Trends & Marketing (Data Konteks Pemasaran)
    * Dataset 3: Strategic Supply Chain Demand Forecasting Dataset (Data Ekonomi & Kompetitif Eksternal)
    * [Sertakan logo Kaggle atau ikon dataset jika memungkinkan]
* **Gambaran Umum Setiap Dataset:**
    * Jelaskan secara singkat konten dan peran masing-masing dataset.
    * Tampilkan beberapa baris pertama (head) dari setiap dataset mentah.
* **Temuan Awal (Ringkasan Info & Describe):**
    * Jumlah data (baris).
    * Kolom-kolom kunci.
    * Tipe data.
    * Statistik deskriptif awal (rata-rata, min, max, dll.).
    * [Opsional: Tampilkan ringkasan tabel info/describe yang relevan].

---

### **Slide 4: Analisis Awal Kualitas Data (Initial Data Quality Analysis)**
* **Nilai yang Hilang (Missing Values):**
    * Tampilkan ringkasan jumlah nilai hilang per kolom untuk setiap dataset mentah.
    * Diskusikan kolom mana yang memiliki nilai hilang dan berapa jumlahnya.
* **Baris Duplikat (Duplicate Rows):**
    * Tampilkan jumlah baris duplikat untuk setiap dataset mentah.
    * Konfirmasi apakah ada duplikat atau tidak.
* **Implikasi Temuan Awal:**
    * Jelaskan mengapa temuan ini penting untuk tahap persiapan data selanjutnya.

---

### **Slide 5: Data Ingestion and Merging (Penyerapan dan Penggabungan Data)**
* **Strategi Penggabungan:**
    * Jelaskan proses penggabungan 3 dataset menjadi `master_df`.
    * Sebutkan kunci penggabungan (`Product ID`, `Date`).
    * Jelaskan metode penggabungan (`Left Join`).
* **Hasil Penggabungan:**
    * Tampilkan beberapa baris pertama (`head`) dari `master_df` setelah penggabungan.
    * Tampilkan ringkasan `info()` dari `master_df` setelah penggabungan untuk menunjukkan jumlah kolom baru dan nilai hilang yang muncul.
    * Diskusikan munculnya banyak nilai hilang setelah penggabungan dan mengapa itu terjadi (kemungkinan perbedaan rentang waktu/kunci).

---

### **Slide 6: Metodologi: Pipeline Persiapan Data Komprehensif**
* **Judul Bagian:** Pipeline Persiapan Data 10 Langkah
* **Gambaran Umum:** Jelaskan bahwa data melalui serangkaian 10 langkah untuk dibersihkan, ditransformasi, dan direkayasa fiturnya agar siap untuk pemodelan.

---

### **Slide 7: Langkah 1: Data Cleaning and Type Conversion**
* **Tujuan:** Pembersihan awal dan penyesuaian tipe data.
* **Aksi:**
    * Penanganan Nilai Hilang (setelah merge):
        * Strategi pengisian nilai hilang (misal: "Unknown" untuk kategori, 0/median untuk numerik, False untuk boolean).
        * Justifikasi singkat untuk setiap strategi.
    * Penanganan Data Leakage:
        * Penghapusan kolom `future_demand` dan alasannya.
    * Konversi Tipe Data:
        * Kolom `Date` diubah ke format datetime.
    * Pemeriksaan Duplikat Akhir: Konfirmasi tidak ada duplikat.
* **[Opsional: Tampilkan ringkasan `isnull().sum()` sebelum dan sesudah penanganan]**

---

### **Slide 8: Langkah 2: Column Consolidation and Selection**
* **Tujuan:** Memilih fitur yang relevan dan memisahkan target.
* **Aksi:**
    * Identifikasi dan penghapusan kolom yang tidak relevan/redundant lainnya.
    * Pemisahan data menjadi Fitur (X) dan Target (y).
* **[Tampilkan contoh X.head() dan y.head() jika perlu]**

---

### **Slide 9: Langkah 3: Exploratory Data Analysis (EDA)**
* **Tujuan:** Memahami data secara mendalam melalui visualisasi.
* **Temuan Utama (Sertakan Plot Kunci):**
    * **Tren Permintaan:** Tampilkan plot rata-rata permintaan harian dari waktu ke waktu. Diskusikan tren atau pola musiman.
    * **Distribusi Target:** Tampilkan histogram atau box plot `Demand`. Diskusikan distribusi dan keberadaan outlier.
    * **Hubungan Fitur vs. Target:** Tampilkan plot yang relevan (misal: Price vs Demand, Discount vs Demand, Promotion vs Demand). Diskusikan hubungan yang terlihat.

---

### **Slide 10: Langkah 3: EDA Lanjutan**
* **Temuan Utama (Sertakan Plot Kunci Lanjutan):**
    * **Tren Permintaan per Kategori:** Tampilkan plot rata-rata permintaan harian untuk beberapa kategori teratas. Bandingkan tren antar kategori.
    * **Distribusi Fitur Numerik:** Tampilkan beberapa histogram fitur numerik kunci.
    * **Distribusi Fitur Kategorikal:** Tampilkan beberapa count plot fitur kategorikal kunci.

---

### **Slide 11: Langkah 3: EDA - Analisis Korelasi**
* **Tujuan:** Mengidentifikasi hubungan antar fitur numerik dan target.
* **Visualisasi:** Tampilkan heatmap korelasi fitur numerik dan `Demand`.
* **Temuan:** Diskusikan korelasi yang kuat (positif atau negatif) yang menarik perhatian.

---

### **Slide 12: Langkah 4: Outlier Detection and Treatment**
* **Tujuan:** Mengidentifikasi dan menangani nilai ekstrem pada fitur.
* **Deteksi:** Tampilkan box plot fitur numerik kunci untuk menunjukkan outlier.
* **Perlakuan:**
    * Jelaskan metode Capping (menggunakan IQR).
    * Jelaskan mengapa capping diterapkan pada fitur tetapi tidak pada target (`Demand`).
    * **[Opsional: Tampilkan perbandingan data sebelum dan sesudah capping untuk beberapa baris/kolom].**

---

### **Slide 13: Langkah 5: Feature Engineering I - Time-Based Features**
* **Tujuan:** Mengekstrak informasi dari kolom tanggal.
* **Fitur yang Dibuat:**
    * Tahun, Bulan, Hari, Hari dalam Seminggu, Hari dalam Setahun, Minggu dalam Setahun, Kuartal.
* **Manfaat:** Membantu model menangkap pola musiman dan tren temporal.
* **[Tampilkan contoh beberapa baris dengan fitur waktu baru].**

---

### **Slide 14: Langkah 6: Feature Engineering II - Lag Features**
* **Tujuan:** Menangkap ketergantungan temporal dari permintaan sebelumnya.
* **Fitur yang Dibuat:**
    * `demand_lag_7` (Permintaan 7 hari sebelumnya)
    * `demand_lag_28` (Permintaan 28 hari sebelumnya)
* **Metode:** Menggunakan `groupby('Product ID')` dan `shift()`.
* **Penanganan NaN:** Jelaskan cara penanganan nilai `NaN` awal.
* **Manfaat:** Memberi model informasi tentang riwayat permintaan terkini.
* **[Tampilkan contoh beberapa baris dengan fitur lag baru].**

---

### **Slide 15: Langkah 7: Feature Engineering III - Rolling Window Features**
* **Tujuan:** Menangkap tren jangka pendek dan menghaluskan noise.
* **Fitur yang Dibuat:**
    * `demand_rolling_mean_7` (Rata-rata permintaan 7 hari bergulir)
    * `demand_rolling_mean_28` (Rata-rata permintaan 28 hari bergulir)
* **Metode:** Menggunakan `groupby('Product ID')` dan `rolling().mean()`.
* **Penanganan NaN:** Jelaskan cara penanganan nilai `NaN` awal.
* **Manfaat:** Memberi model informasi tentang perilaku permintaan rata-rata terkini.
* **[Tampilkan contoh beberapa baris dengan fitur rolling window baru].**

---

### **Slide 16: Langkah 8: Categorical Feature Encoding**
* **Tujuan:** Mengubah fitur kategorikal menjadi format numerik.
* **Metode:** One-Hot Encoding (`pd.get_dummies()`).
* **Kolom yang Di-encode:** Sebutkan kolom kategorikal yang di-encode (misal: Store ID, Category, Region, Weather Condition, Seasonality, SUPPLIER, ITEM TYPE).
* **Hasil:** Menambah kolom biner baru.
* **Pertimbangan:** Sebutkan singkat tentang kardinalitas tinggi jika relevan.
* **[Tampilkan contoh beberapa baris yang menunjukkan kolom biner hasil encoding].**

---

### **Slide 17: Langkah 9: Feature Scaling**
* **Tujuan:** Menstandardisasi rentang fitur numerik.
* **Metode:** StandardScaler.
* **Kolom yang Diskalakan:** Sebutkan jenis kolom numerik yang diskalakan (kecuali boolean, pengenal, target, lag/rolling, capped).
* **Manfaat:** Penting untuk algoritma tertentu, mencegah satu fitur mendominasi.
* **Pertimbangan Time Series:** **Ulangi catatan penting bahwa dalam skenario nyata, penskalaan idealnya setelah split data, dilatih hanya pada training set.**
* **[Tampilkan contoh beberapa baris yang menunjukkan nilai fitur numerik yang diskalakan].**

---

### **Slide 18: Langkah 10: Time-Based Data Splitting**
* **Tujuan:** Membagi data secara kronologis untuk pelatihan dan validasi.
* **Metode:** Pembagian berdasarkan `cutoff_date` (misal: 2023-01-01).
* **Set Data:**
    * Training Set (Data sebelum `cutoff_date`)
    * Validation Set (Data pada atau setelah `cutoff_date`)
* **Pentingnya Metode Kronologis:** Menghindari data leakage temporal, memastikan evaluasi yang realistis.
* **Ukuran Set Data:** Tampilkan ukuran akhir `X_train`, `y_train`, `X_val`, `y_val`.

---

### **Slide 19: Ringkasan Data Pasca-Persiapan (Post-Preparation Data Summary)**
* **Judul Bagian:** Ringkasan Data Siap Model
* **Status Data:** Konfirmasi bahwa data telah dibersihkan, direkayasa fiturnya, dan diskalakan.
* **Gambaran Data Akhir:** Tampilkan beberapa baris pertama (`head`) dari `master_df` yang telah selesai diproses.
* **Struktur Data Akhir:** Tampilkan ringkasan `info()` akhir. Soroti jumlah total kolom dan tipe data final.
* **Pemeriksaan Akhir:** Konfirmasi tidak ada nilai hilang (di kolom yang ditangani) dan tidak ada baris duplikat.
* **Penyimpanan Data:** Sebutkan bahwa dataset yang telah disiapkan disimpan ke file CSV.

---

### **Slide 20: Potensi Peningkatan (Optional / Future Work)**
* **Judul Bagian:** Potensi Fitur Tambahan & Peningkatan
* **Saran:** Berikan beberapa ide untuk meningkatkan persiapan data di masa depan (misal: interaksi fitur, fitur kalender lanjutan, penanganan missing values yang lebih canggih, dll.).

---

### **Slide 21: Penutup**
* **Ringkasan Singkat:** Ulangi bahwa pipeline persiapan data telah berhasil diselesaikan dan data siap untuk pemodelan.
* **Terima Kasih:** Ucapkan terima kasih kepada audiens.
* **Sesi Tanya Jawab:** Buka sesi tanya jawab.

---

Kerangka ini memberikan detail untuk setiap slide, memungkinkan Anda mengisi konten spesifik dari analisis dan visualisasi yang telah Anda lakukan. Pastikan untuk menggunakan visualisasi yang jelas dan ringkas di setiap slide EDA. Semoga presentasi Anda berjalan lancar!

## **Chapter 4: Model Training and Evaluation**

This chapter focuses on selecting, training, and evaluating a machine learning model for demand forecasting.

### **4.1. Model Selection**

We will use XGBoost (Extreme Gradient Boosting) for our demand forecasting model due to its strong performance on structured data and its ability to handle various types of features.

### **4.2. Model Training**
"""

# Initialize and train the XGBoost Regressor model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', # Regression task
                             n_estimators=1000,          # Number of boosting rounds
                             learning_rate=0.05,         # Step size shrinkage
                             max_depth=7,                # Maximum depth of a tree
                             random_state=42,            # For reproducibility
                             n_jobs=-1)                  # Use all available cores

print("Training XGBoost model...")
xgb_model.fit(X_train, y_train)
print("XGBoost model training complete.")

"""### **4.3. Model Evaluation**

Evaluate the trained model using appropriate regression metrics on the validation set.
"""

# Make predictions on the validation set
y_pred = xgb_model.predict(X_val)

# Evaluate the model
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
mae = mean_absolute_error(y_val, y_pred)

print(f"Model Evaluation on Validation Set:")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")

"""### **4.4. Feature Importance**

Analyze the importance of different features in the trained model.
"""

# Get feature importances from the trained model
feature_importances = xgb_model.feature_importances_

# Create a DataFrame of feature importances
feature_importance_df = pd.DataFrame({'feature': X_train.columns, 'importance': feature_importances})

# Sort features by importance
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

# Display top N features
print("\nTop 15 Feature Importances:")
display(feature_importance_df.head(15))

# Plot feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=feature_importance_df.head(15))
plt.title('Top 15 Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

"""## **Chapter 5: Model Evaluation**

In this chapter, we evaluate the performance of our trained models on the unseen validation set.

### **5.1. Make Predictions**
"""

# Make predictions with all three models on X_val
y_pred_xgb = xgb_model.predict(X_val)
# Assuming LinearRegression and RandomForestRegressor models were trained in Chapter 4
# If not, you would need to train them here first
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_val)

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_val)

print("Predictions made with XGBoost model.")
print("Predictions made with Linear Regression model.")
print("Predictions made with Random Forest Regressor model.")

"""### **5.2. Compare Performance**"""

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# Calculate RMSE and MAE for each model
rmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))
mae_xgb = mean_absolute_error(y_val, y_pred_xgb)

# # Assuming y_pred_lr and y_pred_rf are available from the previous cell
rmse_lr = np.sqrt(mean_squared_error(y_val, y_pred_lr))
mae_lr = mean_absolute_error(y_val, y_pred_lr)

rmse_rf = np.sqrt(mean_squared_error(y_val, y_pred_rf))
mae_rf = mean_absolute_error(y_val, y_pred_rf)


# Present the results clearly in a pandas DataFrame
results_df = pd.DataFrame({
    'Model': ['XGBoost', 'Linear Regression', 'Random Forest'], # Add 'Linear Regression', 'Random Forest' if those models were trained
    'RMSE': [rmse_xgb, rmse_lr, rmse_rf], # Add rmse_lr, rmse_rf
    'MAE': [mae_xgb, mae_lr, mae_rf] # Add mae_lr, mae_rf
})

# results_df = pd.DataFrame({
#     'Model': ['XGBoost', 'Random Forest'], # Add 'Linear Regression', 'Random Forest' if those models were trained
#     'RMSE': [rmse_xgb, rmse_rf], # Add rmse_lr, rmse_rf
#     'MAE': [mae_xgb, mae_rf] # Add mae_lr, mae_rf
# })

print("Model Performance Comparison:")
display(results_df)

"""## **Chapter 6: Analysis of the Best Model**

Based on the evaluation metrics, we select the best-performing model and conduct a deeper analysis. Since XGBoost generally performs well, we will focus on its analysis here.

### **6.1. Visualize Predictions vs. Actuals**
"""

# Create a DataFrame for plotting
plot_df = pd.DataFrame({'Actual': y_val, 'Predicted': y_pred_xgb}, index=X_val.index)

# Sort by Date for a meaningful time series plot
plot_df = plot_df.sort_index()

plt.figure(figsize=(15, 6))
plt.plot(val_df['Date'], plot_df['Actual'], label='Actual Demand', alpha=0.7)
plt.plot(val_df['Date'], plot_df['Predicted'], label='Predicted Demand', alpha=0.7)
plt.title('XGBoost Predictions vs. Actuals Over Time')
plt.xlabel('Date')
plt.ylabel('Demand')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""###**6.2. Feature Importance Analysis**"""

# Get feature importances from the trained XGBoost model
feature_importances = xgb_model.feature_importances_

# Create a DataFrame of feature importances
feature_importance_df = pd.DataFrame({'feature': X_train.columns, 'importance': feature_importances})

# Sort features by importance
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

# Display top 20 features
print("\nTop 20 Feature Importances (XGBoost):")
display(feature_importance_df.head(20))

# Plot feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=feature_importance_df.head(20))
plt.title('Top 20 Feature Importances (XGBoost)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

"""## **Chapter 7: Conclusion and Recommendations**

This chapter summarizes the project's findings, provides actionable business recommendations based on the model's insights, and outlines potential areas for future work to further improve demand forecasting accuracy.

### **7.1. Final Summary**

The objective of this project was to develop a machine learning pipeline for daily product demand forecasting using multiple retail datasets. The pipeline involved comprehensive data ingestion, merging, and a 10-step data preparation process including feature engineering and scaling.

Three regression models were trained and evaluated on a chronologically split validation set: XGBoost, Linear Regression, and Random Forest.

Based on the evaluation metrics on the validation set:

*   **XGBoost Model:**
    *   Root Mean Squared Error (RMSE): 13.49
    *   Mean Absolute Error (MAE): 9.99

*   **Linear Regression Model:**
    *   Root Mean Squared Error (RMSE): 21.15
    *   Mean Absolute Error (MAE): 15.77

*   **Random Forest Model:**
    *   Root Mean Squared Error (RMSE): 17.08
    *   Mean Absolute Error (MAE): 12.85

The **XGBoost model** demonstrated the best performance, achieving the lowest RMSE and MAE scores on the validation set. This indicates that the XGBoost model is the most accurate among the tested models for forecasting daily product demand in this context.

### **7.2. Business Recommendations**

The feature importance analysis of the best-performing XGBoost model provides valuable insights for business strategy and inventory management:

*   **Dominance of Sales and Inventory Metrics:** Features directly related to recent sales activity (`Units Sold`, `Units Sold_capped`, `Units Ordered`) and current stock levels (`Inventory Level`) are the most significant drivers of forecasted demand. This highlights the strong influence of immediate historical performance and stock availability on near-future demand.
    *   **Recommendation:** Maintain accurate, real-time tracking of units sold, units ordered, and inventory levels. These metrics are critical inputs for effective forecasting and should be prioritized for data quality. Rapid response strategies based on recent sales trends are likely to be effective.

*   **Impact of Promotions:** `Promotion` is identified as a significant feature. This confirms that promotional activities have a direct and substantial impact on increasing demand.
    *   **Recommendation:** Continue to leverage strategic promotions to stimulate demand. Analyze the effectiveness of different types and timings of promotions based on their correlation with actual sales outcomes. The model can potentially be used to forecast the impact of planned promotions.

*   **Influence of Lagged and Rolling Demand:** Lag features (`demand_lag_7`) and rolling window features (`demand_rolling_mean_7`) are important predictors. This underscores the temporal dependency of demand – past demand patterns are strong indicators of future demand.
    *   **Recommendation:** Incorporate recent historical demand trends into planning. Be mindful of weekly seasonality (captured by `demand_lag_7`) and short-term demand fluctuations (captured by `demand_rolling_mean_7`) when making inventory decisions.

*   **Category and Product Specificity:** Specific product categories (`Category_Furniture`, `Category_Groceries`, `Category_Clothing`, `Category_Electronics`, `Category_Toys`) and even individual products (`Product ID_P0013`, `Product ID_P0002`, etc.) show notable importance. Demand patterns vary significantly by product type.
    *   **Recommendation:** Develop category and product-specific forecasting strategies where appropriate. Tailor inventory management and marketing efforts based on the distinct demand characteristics of different product groups.

*   **Environmental and External Factors:** Features like `Weather Condition_Sunny` and `Epidemic` also contribute to demand prediction.
    *   **Recommendation:** Monitor external factors such as weather forecasts and public health situations, as they can influence consumer behavior and demand. Incorporate these insights into dynamic adjustments of forecasts and inventory.

By focusing on these key drivers identified by the model, the retail company can make more informed decisions regarding inventory, marketing, and supply chain operations, ultimately leading to improved efficiency and reduced costs.

### **7.3. Future Work**

To further enhance the demand forecasting model and pipeline, the following areas are recommended for future work:

*   **Hyperparameter Tuning:** Systematically tune the hyperparameters of the XGBoost model (and potentially other models) using techniques like Grid Search or Randomized Search with cross-validation to find the optimal configuration for better performance.
*   **Explore More Advanced Models:** Investigate other time-series forecasting models, such as ARIMA, Prophet, or deep learning models like LSTMs or GRUs, which can be particularly effective in capturing complex temporal patterns.
*   **Additional Feature Engineering:**
    *   Create more sophisticated lag and rolling window features (e.g., different window sizes, standard deviations, minimums, maximums).
    *   Incorporate calendar features (e.g., is\_weekend, is\_holiday, days until next holiday).
    *   Engineer interaction features between product/store/category and time-based features.
*   **Incorporate External Data:** If available, integrate additional external data sources that could influence demand, such as:
    *   Local events or promotions data.
    *   Economic indicators specific to the region or market.
    *   Social media trends or search interest data related to products.
*   **Cross-Validation Strategy:** Implement a more robust time-series cross-validation strategy (e.g., rolling origin validation) during model training and evaluation to get a more reliable estimate of model performance on unseen future data.
*   **Anomaly Detection:** Implement anomaly detection techniques to identify and potentially treat unusual spikes or drops in demand that might skew the model.
*   **Model Interpretability:** While XGBoost provides feature importance, explore other methods for model interpretability (e.g., SHAP values) to gain a deeper understanding of how individual features influence specific predictions.
*   **Deployment:** Develop a strategy for deploying the trained model to make real-time or near real-time demand forecasts and integrate it into the company's inventory management and planning systems.
"""