{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPsSmLhWeJGL8PSrC5NHvMm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LatiefDataVisionary/demand-forecasting-retail-college-task/blob/main/notebooks/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "810d4a67"
      },
      "source": [
        "# **Demand Forecasting for Retail: A Machine Learning Pipeline Approach**\n",
        "\n",
        "## **Project Overview**\n",
        "\n",
        "**College Course:** Model Development Engineering\n",
        "\n",
        "**Objective:** To develop a robust machine learning model to accurately forecast daily product demand for a retail company. This involves a comprehensive data pipeline, including merging multiple data sources, extensive feature engineering, and model comparison to optimize inventory management and reduce costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98878728"
      },
      "source": [
        "## **Chapter 1: Project Setup**\n",
        "\n",
        "This chapter covers the initial setup, including importing necessary libraries and loading the datasets from their sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da82105f"
      },
      "source": [
        "### 1.1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2402d38"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import xgboost as xgb\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "429ab218"
      },
      "source": [
        "### 1.2. Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27b70eea"
      },
      "source": [
        "data1 = 'https://raw.githubusercontent.com/LatiefDataVisionary/demand-forecasting-retail-college-task/refs/heads/main/data/raw/Dataset%201_Retail%20Store%20Inventory%20and%20Demand%20Forecasting.csv'\n",
        "data2 = 'https://raw.githubusercontent.com/LatiefDataVisionary/demand-forecasting-retail-college-task/refs/heads/main/data/raw/Dataset%202_Retail%20Sales%20Data%20with%20Seasonal%20Trends%20%26%20Marketing.csv'\n",
        "data3 = 'https://raw.githubusercontent.com/LatiefDataVisionary/demand-forecasting-retail-college-task/refs/heads/main/data/raw/Dataset%203_Strategic%20Supply%20Chain%20Demand%20Forecasting%20Dataset.csv'\n",
        "\n",
        "df1 = pd.read_csv(data1)\n",
        "df2 = pd.read_csv(data2)\n",
        "df3 = pd.read_csv(data3)\n",
        "\n",
        "print(\"df1 loaded successfully.\")\n",
        "print(\"df2 loaded successfully.\")\n",
        "print(\"df3 loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c8a79fb"
      },
      "source": [
        "## Chapter 2: Data Ingestion and Merging\n",
        "\n",
        "The first step in our pipeline is to integrate the three disparate datasets into a single, unified master DataFrame that will serve as the foundation for our analysis and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dcbc6cc"
      },
      "source": [
        "### 2.1. Prepare Datasets for Merging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88bcaff9"
      },
      "source": [
        "Standardize column names for consistency before merging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed176a8e"
      },
      "source": [
        "# Rename columns in df2 and df3\n",
        "df2 = df2.rename(columns={'ITEM CODE': 'Product ID'})\n",
        "df3 = df3.rename(columns={'date': 'Date', 'product_id': 'Product ID'})\n",
        "\n",
        "# Convert Product ID in df2 to string to match df1 and df3\n",
        "df2['Product ID'] = df2['Product ID'].astype(str)\n",
        "df1['Product ID'] = df1['Product ID'].astype(str)\n",
        "df3['Product ID'] = df3['Product ID'].astype(str)\n",
        "\n",
        "\n",
        "print(\"Column names standardized and Product ID data types converted.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58479dd1"
      },
      "source": [
        "### 2.2. Merge Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f01908ea"
      },
      "source": [
        "Perform a two-step merge process to create the `master_df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77d1f4a1"
      },
      "source": [
        "# Merge df1 with df2\n",
        "master_df = pd.merge(df1, df2[['Product ID', 'SUPPLIER', 'ITEM TYPE']], on='Product ID', how='left')\n",
        "\n",
        "# Merge the result with df3\n",
        "master_df = pd.merge(master_df, df3[['Date', 'Product ID', 'holiday_season', 'promotion_applied',\n",
        "                                     'competitor_price_index', 'economic_index', 'weather_impact',\n",
        "                                     'price', 'discount_percentage', 'sales_revenue', 'region_Europe',\n",
        "                                     'region_North America', 'store_type_Retail', 'store_type_Wholesale',\n",
        "                                     'category_Cabinets', 'category_Chairs', 'category_Sofas',\n",
        "                                     'category_Tables', 'future_demand']],\n",
        "                     on=['Date', 'Product ID'], how='left')\n",
        "\n",
        "print(\"Datasets merged successfully.\")\n",
        "display(master_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d73d7b00"
      },
      "source": [
        "## Chapter 3: The 10-Step Data Preparation Pipeline\n",
        "\n",
        "This chapter details the comprehensive, 10-step data preparation and cleaning process required to transform the raw, merged data into a feature-rich, model-ready format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f15c32a"
      },
      "source": [
        "### Step 1: Data Cleaning and Type Conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78e3e0e"
      },
      "source": [
        "Inspect the Master DataFrame and handle missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5c5d8d8"
      },
      "source": [
        "# Inspect the Master DataFrame\n",
        "print(\"Info of master_df:\")\n",
        "master_df.info()\n",
        "\n",
        "print(\"\\nHead of master_df:\")\n",
        "display(master_df.head())\n",
        "\n",
        "# Handle Missing Values\n",
        "print(\"\\nMissing values before handling:\")\n",
        "print(master_df.isnull().sum().to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Fill missing 'SUPPLIER' with \"Unknown\"\n",
        "master_df['SUPPLIER'] = master_df['SUPPLIER'].fillna('Unknown')\n",
        "\n",
        "# Fill missing numerical columns with the median\n",
        "numerical_cols_with_missing = ['RETAIL SALES', 'holiday_season', 'promotion_applied',\n",
        "                               'competitor_price_index', 'economic_index', 'weather_impact',\n",
        "                               'price', 'discount_percentage', 'sales_revenue', 'future_demand']\n",
        "\n",
        "for col in numerical_cols_with_missing:\n",
        "    if master_df[col].isnull().any():\n",
        "        median_val = master_df[col].median()\n",
        "        master_df[col] = master_df[col].fillna(median_val)\n",
        "        print(f\"Filled missing values in {col} with median ({median_val}).\")\n",
        "\n",
        "\n",
        "print(\"\\nMissing values after handling:\")\n",
        "print(master_df.isnull().sum().to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Convert Date column to datetime object\n",
        "master_df['Date'] = pd.to_datetime(master_df['Date'])\n",
        "print(\"\\n'Date' column converted to datetime.\")\n",
        "\n",
        "# Check for Duplicates\n",
        "print(\"\\nDuplicate rows in master_df:\")\n",
        "print(master_df.duplicated().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dc40ca9"
      },
      "source": [
        "### Step 2: Column Consolidation and Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2078f90"
      },
      "source": [
        "Identify and drop redundant or irrelevant columns. Select the target variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "447f23c8"
      },
      "source": [
        "# Identify and Drop Redundant Columns\n",
        "# 'future_demand' is a data leak and must be dropped.\n",
        "# 'ITEM DESCRIPTION' might be redundant given 'Product ID'.\n",
        "columns_to_drop = ['future_demand', 'ITEM DESCRIPTION']\n",
        "master_df = master_df.drop(columns=columns_to_drop)\n",
        "\n",
        "print(f\"Dropped redundant columns: {columns_to_drop}\")\n",
        "\n",
        "# Target Variable Selection\n",
        "target_variable = 'Demand'\n",
        "y = master_df[target_variable]\n",
        "X = master_df.drop(columns=[target_variable])\n",
        "\n",
        "print(f\"Target variable '{target_variable}' selected.\")\n",
        "print(\"Features DataFrame (X) created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "889f1082"
      },
      "source": [
        "### Step 3: Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dec87f3d"
      },
      "source": [
        "Visualize key aspects of the data to understand trends, distributions, and relationships."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a83af13d"
      },
      "source": [
        "# Time Series Analysis: Plot daily average Demand\n",
        "daily_demand = master_df.groupby('Date')['Demand'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.lineplot(data=daily_demand, x='Date', y='Demand')\n",
        "plt.title('Daily Average Demand Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Demand')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Distribution Analysis\n",
        "# Histograms for numerical features\n",
        "numerical_features = ['Price', 'Inventory Level', 'Units Sold', 'Demand', 'RETAIL SALES',\n",
        "                      'competitor_price_index', 'economic_index', 'price', 'discount_percentage',\n",
        "                      'sales_revenue']\n",
        "\n",
        "master_df[numerical_features].hist(figsize=(15, 10), bins=30)\n",
        "plt.suptitle('Histograms of Numerical Features', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Count plots for categorical features\n",
        "categorical_features = ['Category', 'Region', 'Store ID', 'Weather Condition', 'Seasonality', 'ITEM TYPE']\n",
        "\n",
        "for col in categorical_features:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.countplot(data=master_df, y=col, order=master_df[col].value_counts().index)\n",
        "    plt.title(f'Count Plot of {col}')\n",
        "    plt.xlabel('Count')\n",
        "    plt.ylabel(col)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Correlation Analysis for numerical features\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = master_df[numerical_features].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72fc2023"
      },
      "source": [
        "### Step 4: Outlier Detection and Treatment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dbbdfe7"
      },
      "source": [
        "Visualize potential outliers and provide an example of outlier treatment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1644cb77"
      },
      "source": [
        "# Visualize Outliers using Box Plots\n",
        "outlier_features = ['Units Sold', 'Price', 'Demand', 'Inventory Level', 'RETAIL SALES']\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "master_df[outlier_features].boxplot()\n",
        "plt.title('Box Plots for Outlier Detection')\n",
        "plt.ylabel('Value')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Treat Outliers (Optional but Recommended)\n",
        "# Explain the concept of capping outliers using the IQR method.\n",
        "print(\"Outlier Treatment (Capping using IQR method):\")\n",
        "print(\"The Interquartile Range (IQR) method can be used to identify outliers.\")\n",
        "print(\"Values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR can be considered outliers.\")\n",
        "print(\"Capping involves replacing these outliers with the boundary values (Q1 - 1.5*IQR or Q3 + 1.5*IQR).\")\n",
        "print(\"Below is commented-out code demonstrating how to cap outliers for 'Units Sold' as an example:\")\n",
        "\n",
        "# Example commented-out code for capping 'Units Sold' outliers:\n",
        "# Q1 = master_df['Units Sold'].quantile(0.25)\n",
        "# Q3 = master_df['Units Sold'].quantile(0.75)\n",
        "# IQR = Q3 - Q1\n",
        "# lower_bound = Q1 - 1.5 * IQR\n",
        "# upper_bound = Q3 + 1.5 * IQR\n",
        "# master_df['Units Sold_capped'] = master_df['Units Sold'].clip(lower=lower_bound, upper=upper_bound)\n",
        "# print(\"\\nExample: 'Units Sold' capped (new column 'Units Sold_capped' created).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3e58b02"
      },
      "source": [
        "### Step 5: Feature Engineering I - Time-Based Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97e2740c"
      },
      "source": [
        "Create new features extracted from the 'Date' column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76cc87c2"
      },
      "source": [
        "# Create time-based features\n",
        "master_df['year'] = master_df['Date'].dt.year\n",
        "master_df['month'] = master_df['Date'].dt.month\n",
        "master_df['day'] = master_df['Date'].dt.day\n",
        "master_df['dayofweek'] = master_df['Date'].dt.dayofweek # Monday=0, Sunday=6\n",
        "master_df['dayofyear'] = master_df['Date'].dt.dayofyear\n",
        "master_df['weekofyear'] = master_df['Date'].dt.isocalendar().week.astype(int)\n",
        "master_df['quarter'] = master_df['Date'].dt.quarter\n",
        "\n",
        "print(\"Time-based features created.\")\n",
        "display(master_df[['Date', 'year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca10981c"
      },
      "source": [
        "### Step 6: Feature Engineering II - Lag Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a27750de"
      },
      "source": [
        "Lag features are crucial for time-series forecasting as they represent past values of the target variable, capturing temporal dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70514cd5"
      },
      "source": [
        "# Create lag features for the 'Demand' column\n",
        "master_df['demand_lag_7'] = master_df.groupby('Product ID')['Demand'].shift(7)\n",
        "master_df['demand_lag_28'] = master_df.groupby('Product ID')['Demand'].shift(28)\n",
        "\n",
        "# Handle resulting NaN values (e.g., fill with 0 or the mean of the lag feature)\n",
        "# Filling with 0 as these are the initial periods where no lag data is available\n",
        "master_df['demand_lag_7'] = master_df['demand_lag_7'].fillna(0)\n",
        "master_df['demand_lag_28'] = master_df['demand_lag_28'].fillna(0)\n",
        "\n",
        "\n",
        "print(\"Lag features created for 'Demand'.\")\n",
        "display(master_df[['Date', 'Product ID', 'Demand', 'demand_lag_7', 'demand_lag_28']].head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b9e34cc"
      },
      "source": [
        "### Step 7: Feature Engineering III - Rolling Window Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b40be5c"
      },
      "source": [
        "Rolling window features, such as rolling means, help smooth out noise and capture short-term trends in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9ac0c27"
      },
      "source": [
        "# Create rolling mean features for 'Demand'\n",
        "master_df['demand_rolling_mean_7'] = master_df.groupby('Product ID')['Demand'].transform(lambda x: x.rolling(window=7).mean())\n",
        "master_df['demand_rolling_mean_28'] = master_df.groupby('Product ID')['Demand'].transform(lambda x: x.rolling(window=28).mean())\n",
        "\n",
        "# Handle resulting NaN values (initial periods)\n",
        "# Filling with the mean of the rolling window feature after calculation\n",
        "master_df['demand_rolling_mean_7'] = master_df['demand_rolling_mean_7'].fillna(master_df['demand_rolling_mean_7'].mean())\n",
        "master_df['demand_rolling_mean_28'] = master_df['demand_rolling_mean_28'].fillna(master_df['demand_rolling_mean_28'].mean())\n",
        "\n",
        "\n",
        "print(\"Rolling window features created for 'Demand'.\")\n",
        "display(master_df[['Date', 'Product ID', 'Demand', 'demand_rolling_mean_7', 'demand_rolling_mean_28']].head(30)) # Display more rows to see non-NaN values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17da0191"
      },
      "source": [
        "### Step 8: Categorical Feature Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54cfe6f4"
      },
      "source": [
        "Convert categorical features into a numerical format using One-Hot Encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb2fe3bf"
      },
      "source": [
        "# Identify categorical columns (excluding 'Date' and 'Product ID' which are not features for encoding here)\n",
        "categorical_features_for_encoding = master_df.select_dtypes(include='object').columns.tolist()\n",
        "# Remove columns that are not intended for one-hot encoding or have been handled\n",
        "categorical_features_for_encoding.remove('Store ID') # Assuming Store ID will be used as is or dropped later if not needed\n",
        "# Note: 'Category', 'Region', 'Weather Condition', 'Seasonality', 'SUPPLIER', 'ITEM TYPE' are good candidates\n",
        "\n",
        "print(f\"Categorical features to encode: {categorical_features_for_encoding}\")\n",
        "\n",
        "# Apply One-Hot Encoding\n",
        "master_df = pd.get_dummies(master_df, columns=categorical_features_for_encoding, dummy_na=False)\n",
        "\n",
        "print(\"Categorical features encoded using One-Hot Encoding.\")\n",
        "display(master_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c8ec295"
      },
      "source": [
        "### Step 9: Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26b22b11"
      },
      "source": [
        "Scale numerical features to standardize their range, which is important for many machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cefdf526"
      },
      "source": [
        "# Identify numerical features to scale\n",
        "# Exclude 'Date', 'Product ID', the target variable 'Demand', and the newly created binary/boolean columns from one-hot encoding\n",
        "numerical_features_to_scale = X.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Remove boolean columns resulting from one-hot encoding if they are still in X\n",
        "boolean_cols = X.select_dtypes(include='bool').columns.tolist()\n",
        "numerical_features_to_scale = [col for col in numerical_features_to_scale if col not in boolean_cols]\n",
        "\n",
        "# Exclude lag and rolling features as they are already derived from scaled data or handled\n",
        "lag_rolling_features = ['demand_lag_7', 'demand_lag_28', 'demand_rolling_mean_7', 'demand_rolling_mean_28']\n",
        "numerical_features_to_scale = [col for col in numerical_features_to_scale if col not in lag_rolling_features]\n",
        "\n",
        "print(f\"Numerical features to scale: {numerical_features_to_scale}\")\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "master_df[numerical_features_to_scale] = scaler.fit_transform(master_df[numerical_features_to_scale])\n",
        "\n",
        "print(\"Numerical features scaled using StandardScaler.\")\n",
        "display(master_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40d000b3"
      },
      "source": [
        "### Step 10: Time-Based Data Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17f5a195"
      },
      "source": [
        "For time-series data, it is critical to split the data chronologically to avoid data leakage from the future into the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69cd5e81"
      },
      "source": [
        "# Define the chronological cutoff date for splitting\n",
        "# Using a date in early 2023 as an example cutoff\n",
        "cutoff_date = pd.to_datetime('2023-01-01')\n",
        "\n",
        "# Split data based on the cutoff date\n",
        "train_df = master_df[master_df['Date'] < cutoff_date].copy()\n",
        "val_df = master_df[master_df['Date'] >= cutoff_date].copy()\n",
        "\n",
        "# Define features (X) and target (y) for train and validation sets\n",
        "# Exclude 'Date' and 'Product ID' as they are not features for the model\n",
        "features = [col for col in master_df.columns if col not in ['Date', 'Product ID', target_variable]]\n",
        "\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_variable]\n",
        "\n",
        "X_val = val_df[features]\n",
        "y_val = val_df[target_variable]\n",
        "\n",
        "\n",
        "print(f\"Data split into training and validation sets based on cutoff date: {cutoff_date}\")\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Validation set shape: {X_val.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ad9307f"
      },
      "source": [
        "## Chapter 4: Model Training and Evaluation\n",
        "\n",
        "This chapter focuses on selecting, training, and evaluating a machine learning model for demand forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d75afd4a"
      },
      "source": [
        "### 4.1. Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ef4a40"
      },
      "source": [
        "We will use XGBoost (Extreme Gradient Boosting) for our demand forecasting model due to its strong performance on structured data and its ability to handle various types of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ebc231f"
      },
      "source": [
        "### 4.2. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6168b3c"
      },
      "source": [
        "# Initialize and train the XGBoost Regressor model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', # Regression task\n",
        "                             n_estimators=1000,          # Number of boosting rounds\n",
        "                             learning_rate=0.05,         # Step size shrinkage\n",
        "                             max_depth=7,                # Maximum depth of a tree\n",
        "                             random_state=42,            # For reproducibility\n",
        "                             n_jobs=-1)                  # Use all available cores\n",
        "\n",
        "print(\"Training XGBoost model...\")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "print(\"XGBoost model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c482847"
      },
      "source": [
        "### 4.3. Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fb55ac"
      },
      "source": [
        "Evaluate the trained model using appropriate regression metrics on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07de8ae7"
      },
      "source": [
        "# Make predictions on the validation set\n",
        "y_pred = xgb_model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "print(f\"Model Evaluation on Validation Set:\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96678b2d"
      },
      "source": [
        "### 4.4. Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da1e8d1e"
      },
      "source": [
        "Analyze the importance of different features in the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dc53237"
      },
      "source": [
        "# Get feature importances from the trained model\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame of feature importances\n",
        "feature_importance_df = pd.DataFrame({'feature': X_train.columns, 'importance': feature_importances})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Display top N features\n",
        "print(\"\\nTop 15 Feature Importances:\")\n",
        "display(feature_importance_df.head(15))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance_df.head(15))\n",
        "plt.title('Top 15 Feature Importances')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "115bbbd0"
      },
      "source": [
        "## Chapter 5: Model Evaluation\n",
        "\n",
        "In this chapter, we evaluate the performance of our trained models on the unseen validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fd313e4"
      },
      "source": [
        "### 5.1. Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41070cdf"
      },
      "source": [
        "# Make predictions with all three models on X_val\n",
        "y_pred_xgb = xgb_model.predict(X_val)\n",
        "# Assuming LinearRegression and RandomForestRegressor models were trained in Chapter 4\n",
        "# If not, you would need to train them here first\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# lr_model = LinearRegression()\n",
        "# lr_model.fit(X_train, y_train)\n",
        "# y_pred_lr = lr_model.predict(X_val)\n",
        "\n",
        "# rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "# rf_model.fit(X_train, y_train)\n",
        "# y_pred_rf = rf_model.predict(X_val)\n",
        "\n",
        "print(\"Predictions made with XGBoost model.\")\n",
        "# print(\"Predictions made with Linear Regression model.\")\n",
        "# print(\"Predictions made with Random Forest Regressor model.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf2b4a4f"
      },
      "source": [
        "### 5.2. Compare Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2191bcd7"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Calculate RMSE and MAE for each model\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n",
        "mae_xgb = mean_absolute_error(y_val, y_pred_xgb)\n",
        "\n",
        "# Assuming y_pred_lr and y_pred_rf are available from the previous cell\n",
        "# rmse_lr = np.sqrt(mean_squared_error(y_val, y_pred_lr))\n",
        "# mae_lr = mean_absolute_error(y_val, y_pred_lr)\n",
        "\n",
        "# rmse_rf = np.sqrt(mean_squared_error(y_val, y_pred_rf))\n",
        "# mae_rf = mean_absolute_error(y_val, y_pred_rf)\n",
        "\n",
        "\n",
        "# Present the results clearly in a pandas DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['XGBoost'], # Add 'Linear Regression', 'Random Forest' if those models were trained\n",
        "    'RMSE': [rmse_xgb], # Add rmse_lr, rmse_rf\n",
        "    'MAE': [mae_xgb] # Add mae_lr, mae_rf\n",
        "})\n",
        "\n",
        "print(\"Model Performance Comparison:\")\n",
        "display(results_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "981bb893"
      },
      "source": [
        "## Chapter 6: Analysis of the Best Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d40ea08"
      },
      "source": [
        "Based on the evaluation metrics, we select the best-performing model and conduct a deeper analysis. Since XGBoost generally performs well, we will focus on its analysis here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "254be1fd"
      },
      "source": [
        "### 6.1. Visualize Predictions vs. Actuals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d91114c7"
      },
      "source": [
        "# Create a DataFrame for plotting\n",
        "plot_df = pd.DataFrame({'Actual': y_val, 'Predicted': y_pred_xgb}, index=X_val.index)\n",
        "\n",
        "# Sort by Date for a meaningful time series plot\n",
        "plot_df = plot_df.sort_index()\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(val_df['Date'], plot_df['Actual'], label='Actual Demand', alpha=0.7)\n",
        "plt.plot(val_df['Date'], plot_df['Predicted'], label='Predicted Demand', alpha=0.7)\n",
        "plt.title('XGBoost Predictions vs. Actuals Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Demand')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f900a44"
      },
      "source": [
        "### 6.2. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29187c61"
      },
      "source": [
        "# Get feature importances from the trained XGBoost model\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame of feature importances\n",
        "feature_importance_df = pd.DataFrame({'feature': X_train.columns, 'importance': feature_importances})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Display top 20 features\n",
        "print(\"\\nTop 20 Feature Importances (XGBoost):\")\n",
        "display(feature_importance_df.head(20))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance_df.head(20))\n",
        "plt.title('Top 20 Feature Importances (XGBoost)')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}